## 一、python虚拟运行环境

### 1. 安装Miniconda

Windows 系统安装步骤
1. 下载 Miniconda 安装包

   - 访问官方下载页面：https://docs.conda.io/projects/miniconda/en/latest/
   - 选择 Windows 版本的安装包（根据系统选择 64位/32位）
2. 运行安装程序

   - 双击下载的安装文件（例如：Miniconda3-latest-Windows-x86_64.exe）
   - 建议安装时选择"仅为当前用户安装"
3. 验证安装是否成功：
   ```bash
   # 打开命令提示符或 PowerShell，输入：
   conda --version
   ```


Linux/MacOS 系统安装步骤：
1. 下载安装脚本：
   ```bash
   # Linux
   wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh
   
   # MacOS
   wget https://repo.anaconda.com/miniconda/Miniconda3-latest-MacOSX-x86_64.sh
   ```

2. 运行安装脚本：
   ```bash
   # Linux
   bash Miniconda3-latest-Linux-x86_64.sh
   
   # MacOS
   bash Miniconda3-latest-MacOSX-x86_64.sh
   ```

3. 初始化 conda：
   ```bash
   source ~/.bashrc  # Linux
   source ~/.zshrc   # MacOS
   ```

### 2. 验证安装
   ```bash
   # 验证 conda 版本
   conda --version
   
   # 验证 conda 环境
   conda info
   
   # 更新 conda 到最新版本
   conda update conda
   ```

### 3. 搭建虚拟运行环境
   ```bash
   # 创建新的虚拟环境
   conda create -n rag python=3.11
   
   # 激活虚拟环境
   conda activate rag
   
   # 查看当前环境列表
   conda env list
   ```

### 4. 安装项目依赖
   ```bash
   # 进入项目依赖requirements_XXX.txt所在目录执行
   pip install -r requirements_XXX.txt
   ```

注意事项：
- 安装过程中请保持网络连接稳定
- Windows 用户建议使用管理员权限运行命令提示符
- 如果下载速度慢，可以考虑使用国内镜像源
- 建议在安装完成后重启终端


## 二、基础大模型

### 1. 通过API访问云端商业模型

#### 1. GPT国内代理
  - [https://www.apiyi.com/](https://www.apiyi.com/)
  - [https://2233.ai/api](https://2233.ai/api)
  - [https://www.eylink.cn/](https://www.eylink.cn/)
    
   - 如果是自己购买的三方openai代理, 诸如在 https://www.apiyi.com/ 上购买的，可以用
      ``` python
         from openai import OpenAI
         
         client = OpenAI(
             api_key="YOUR_API_KEY",
             base_url="https://vip.apiyi.com/v1"
         )
         
         completion = client.chat.completions.create(
             model="gpt-4o",
             messages=[
                 {"role": "user", "content": "write a haiku about ai"}
             ]
         )
      ```
   - 如果是自己购买的三方openai代理, 诸如在 https://www.eylink.cn/ 上购买的，可以用
      ``` python
         client = OpenAI(
           base_url=' https://gtapi.xiaoerchaoren.com:8932/v1'
         )
      ```
   - 如果是自己购买的三方openai代理, 诸如在 https://2233.ai/api 上购买的，可以用
      ``` python
         client = OpenAI(
           base_url=' https://api.gptsapi.net/v1'
         )
      ```

#### 2. OpenAI API使用国内的智谱或DeepSeek
   ``` python
      # OpenAI 客户端调用
      from openai import Client
      
      # 创建 OpenAI 客户端
      openai = OpenAI()
      
      # OpenAI API调用（代理方式）
      openai = OpenAI(
           api_key="XXX", 
           base_url="https://vip.apiyi.com/v1"
      )
      
      # 智谱API调用(glm-4v-flash) 
      openai = OpenAI(
          api_key="XXX",
          base_url="https://open.bigmodel.cn/api/paas/v4/"
      )
      
      # DeepSeek API调用（deepseek-chat）
      openai = OpenAI(
           api_key="XXX",
           base_url="https://api.deepseek.com"
      )
      
   ```

- 自行注册智谱或DeepSeek账号后，获取API KEY，代码中进行替换
  - DeepSeek：https://api-docs.deepseek.com/zh-cn/
  - 智谱：https://www.bigmodel.cn/dev/api/normal-model/glm-4v

### 2. 基于Ollama部署本地开源大模型
   #### 1. 安装 Ollama
   
   Windows 系统安装
   1. 下载 Ollama 安装包
      - 访问官方下载页面：https://ollama.com/download
      - 下载 Windows 版本的安装包
   
   2. 运行安装程序
      - 双击下载的安装文件
      - 按照安装向导完成安装
   
   3. 验证安装
      ```bash
      # 打开 PowerShell 或命令提示符，输入：
      ollama --version
      ```

   Linux/MacOS 系统安装
   ```bash
   # 使用 curl 安装
   curl -fsSL https://ollama.com/install.sh | sh
   
   # 验证安装
   ollama --version
   ```
   #### 2. 下载 Qwen2.5 模型
   在下载模型之前，请根据您的硬件配置选择合适的模型版本：
   1. **硬件配置与模型选择**
      - 低配置（8GB内存）：建议使用 Qwen2.5 1.8B 或 4B 版本
      - 中等配置（16GB内存）：可以使用 Qwen2.5 7B 版本
      - 高配置（32GB+内存）：可以使用 Qwen2.5 14B 或更大版本
   3. **模型版本说明**
      - 1.8B：轻量级版本，适合入门和测试
      - 4B：平衡版本，适合一般应用
      - 7B：标准版本，适合大多数应用场景
      - 14B：高性能版本，需要更好的硬件支持
   5. **量化版本选择**
      - 如果内存有限，可以选择量化版本（如 qwen2.5:7b-q4）
      - 量化版本会降低一些性能，但能显著减少内存占用

   ```bash
   # 查看可用的模型版本
   ollama list
   
   # 拉取适合的模型版本（示例）
   ollama pull qwen2.5:7b  # 标准版本
   ollama pull qwen2.5:7b-q4  # 量化版本
   ```
   
   注意：
   - 模型大小约为 7GB（标准版本），请确保有足够的磁盘空间
   - 下载速度取决于网络状况，建议使用稳定的网络连接
   - 如果下载速度较慢，可以考虑使用代理
  
   #### 3. 运行模型
   - 基础运行
      ```bash
      # 启动模型进行对话
      ollama run qwen2.5:7b
      ```

   - 使用 API 接口
      ```bash
      # 启动 Ollama 服务
      ollama serve
      
      # 在另一个终端中使用 curl 测试 API
      curl http://localhost:11434/api/generate -d '{
        "model": "qwen2.5:7b",
        "prompt": "你好，请介绍一下你自己"
      }'
      ```
   
   #### 4. 常用参数配置
   - 运行参数
      ```bash
      # 使用特定参数运行模型
      ollama run qwen2.5:7b --temperature 0.7 --top-p 0.9
      ```
   
   - 常用参数说明：
     - `--temperature`: 控制输出的随机性（0-1）
     - `--top-p`: 控制输出的多样性（0-1）
     - `--num-predict`: 控制生成的最大 token 数
     - `--seed`: 设置随机种子，用于复现结果    
   
   #### 5. 模型管理
      ```bash
      # 查看已安装的模型
      ollama list
      
      # 删除模型
      ollama rm qwen2.5:7b
      
      # 复制模型
      ollama cp qwen2.5:7b qwen2.5:7b-backup
      ```

   #### 6. 进阶使用
   - 使用 Python 调用
     ```python
      import requests
      
      def query_ollama(prompt):
          response = requests.post(
              'http://localhost:11434/api/generate',
              json={
                  'model': 'qwen2.5:7b',
                  'prompt': prompt
              }
          )
          return response.json()
      
      # 使用示例
      result = query_ollama("你好，请介绍一下你自己")
      print(result)
     ```
   - 使用 LangChain 集成
      ```python
      from langchain.llms import Ollama
      
      llm = Ollama(model="qwen2.5:7b")
      response = llm("你好，请介绍一下你自己")
      print(response)
      ``` 
   
   #### 7. 常见问题解决
   1. **内存不足**
      - 确保系统有足够的可用内存（建议至少 16GB）
      - 可以尝试使用量化版本的模型
   
   2. **GPU 加速**
      - 如果使用 NVIDIA GPU，确保安装了 CUDA 驱动
      - Ollama 会自动使用 GPU 加速
   
   3. **网络问题**
      - 如果下载失败，可以尝试使用代理
      - 检查网络连接是否稳定
     
## 三、嵌入模型

### 1. 通过API访问云端嵌入模型
### 2. 基于Ollama部署本地嵌入模型
### 3. 基于TEL部署本地嵌入模型

## 四、向量库

### 1. 安装Chroma


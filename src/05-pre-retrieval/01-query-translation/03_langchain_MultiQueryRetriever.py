"""
ä½¿ç”¨LangChainçš„MultiQueryRetrieverè¿›è¡Œå¤šæŸ¥è¯¢ç”Ÿæˆ
å±•ç¤ºå°†å•ä¸ªæŸ¥è¯¢æ‰©å±•ä¸ºå¤šä¸ªç›¸å…³æŸ¥è¯¢çš„æ•ˆæœ
"""

import logging
import os
from dotenv import load_dotenv
from langchain.retrievers.multi_query import MultiQueryRetriever
from langchain_chroma import Chroma
from langchain_community.document_loaders import TextLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_deepseek import ChatDeepSeek
from langchain_huggingface import HuggingFaceEmbeddings

# åŠ è½½.envæ–‡ä»¶ä¸­çš„ç¯å¢ƒå˜é‡ï¼ŒåŒ…æ‹¬APIå¯†é’¥ç­‰æ•æ„Ÿä¿¡æ¯
load_dotenv()

# è®¾ç½®æ—¥å¿—è®°å½•ï¼ŒæŸ¥çœ‹å¤šæŸ¥è¯¢ç”Ÿæˆè¿‡ç¨‹
logging.basicConfig(level=logging.INFO, format="%(levelname)s: %(message)s")
multi_query_logger = logging.getLogger("langchain.retrievers.multi_query")
multi_query_logger.setLevel(logging.DEBUG)

print("\nğŸ“š æ­£åœ¨åŠ è½½æ–‡æ¡£æ•°æ®...")
# ä½¿ç”¨é¡¹ç›®æ ¹ç›®å½•çš„æ•°æ®æ–‡ä»¶
loader = TextLoader("data/txt/ç³–å°¿ç—….txt", encoding="utf-8")
data = loader.load()
print("âœ… æ–‡æ¡£åŠ è½½å®Œæˆ")

# æ–‡æœ¬åˆ†å—
text_splitter = RecursiveCharacterTextSplitter(chunk_size=300, chunk_overlap=50)
all_splits = text_splitter.split_documents(data)

# å‘é‡å­˜å‚¨
print("\nğŸ”¤ æ­£åœ¨æ„å»ºå‘é‡å­˜å‚¨...")
embed_model = HuggingFaceEmbeddings(model_name="BAAI/bge-small-zh")
vectorstore = Chroma.from_documents(documents=all_splits, embedding=embed_model)
print("âœ… å‘é‡å­˜å‚¨æ„å»ºå®Œæˆ")

# è®¾ç½®LLM
llm = ChatDeepSeek(
    model="deepseek-chat", temperature=0.1, api_key=os.getenv("DEEPSEEK_API_KEY")
)

# åˆ›å»ºå¤šæŸ¥è¯¢æ£€ç´¢å™¨
print("\nğŸ› ï¸ æ­£åœ¨è®¾ç½®æ£€ç´¢å™¨...")
multi_query_retriever = MultiQueryRetriever.from_llm(
    retriever=vectorstore.as_retriever(search_kwargs={"k": 3}), llm=llm
)
print("âœ… æ£€ç´¢å™¨è®¾ç½®å®Œæˆ")

query = "ç³–å°¿ç—…æœ‰ä»€ä¹ˆç—‡çŠ¶ï¼Ÿ"

print(f"\nğŸ” æµ‹è¯•æŸ¥è¯¢: ã€Œ{query}ã€")
print("ğŸ”„ æ­£åœ¨ç”Ÿæˆå¤šä¸ªæŸ¥è¯¢å¹¶æ£€ç´¢...")

# ä½¿ç”¨MultiQueryRetrieverè¿›è¡Œå¤šæŸ¥è¯¢ç”Ÿæˆå’Œæ£€ç´¢
docs = multi_query_retriever.invoke(query)

print(f"\nğŸ“„ æ£€ç´¢åˆ°çš„æ–‡æ¡£ç‰‡æ®µ:")
if isinstance(docs, list):
    for i, doc in enumerate(docs, 1):
        content = doc.page_content if hasattr(doc, "page_content") else str(doc)
        preview = content.replace('\n', ' ').strip()
        if len(preview) > 150:
            preview = preview[:150] + "..."
        print(f"[{i}] {preview}")
else:
    print("âŒ æœªæ‰¾åˆ°ç›¸å…³æ–‡æ¡£")

print(f"\nâœ… å¤šæŸ¥è¯¢æ£€ç´¢å®Œæˆï¼æ£€ç´¢åˆ° {len(docs) if isinstance(docs, list) else 0} ä¸ªç›¸å…³æ–‡æ¡£")
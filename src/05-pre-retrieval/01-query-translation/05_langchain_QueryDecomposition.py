"""
ä½¿ç”¨æŸ¥è¯¢åˆ†è§£æŠ€æœ¯å¤„ç†å¤æ‚æŸ¥è¯¢
å°†å¤æ‚æŸ¥è¯¢æ‹†åˆ†æˆå¤šä¸ªå­é—®é¢˜ï¼Œæä¾›æ›´å…¨é¢çš„æ£€ç´¢ç»“æœ
"""

import logging
import os
from dotenv import load_dotenv
from langchain_chroma import Chroma
from langchain_community.document_loaders import TextLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_deepseek import ChatDeepSeek
from langchain_huggingface import HuggingFaceEmbeddings

# åŠ è½½.envæ–‡ä»¶ä¸­çš„ç¯å¢ƒå˜é‡ï¼ŒåŒ…æ‹¬APIå¯†é’¥ç­‰æ•æ„Ÿä¿¡æ¯
load_dotenv()

# è®¾ç½®æ—¥å¿—è®°å½•
logging.basicConfig(level=logging.INFO, format="%(levelname)s: %(message)s")

print("\nğŸ“š æ­£åœ¨åŠ è½½æ–‡æ¡£æ•°æ®...")
# ä½¿ç”¨é¡¹ç›®æ ¹ç›®å½•çš„æ•°æ®æ–‡ä»¶
loader = TextLoader("data/txt/ç³–å°¿ç—….txt", encoding="utf-8")
data = loader.load()
print("âœ… æ–‡æ¡£åŠ è½½å®Œæˆ")

# æ–‡æœ¬åˆ†å—
text_splitter = RecursiveCharacterTextSplitter(chunk_size=300, chunk_overlap=50)
all_splits = text_splitter.split_documents(data)

# å‘é‡å­˜å‚¨
print("\nğŸ”¤ æ­£åœ¨æ„å»ºå‘é‡å­˜å‚¨...")
embed_model = HuggingFaceEmbeddings(model_name="BAAI/bge-small-zh")
vectorstore = Chroma.from_documents(documents=all_splits, embedding=embed_model)
print("âœ… å‘é‡å­˜å‚¨æ„å»ºå®Œæˆ")

# è®¾ç½®LLM
llm = ChatDeepSeek(
    model="deepseek-chat", temperature=0, api_key=os.getenv("DEEPSEEK_API_KEY")
)

# åˆ›å»ºåŸºç¡€æ£€ç´¢å™¨
print("\nğŸ› ï¸ æ­£åœ¨è®¾ç½®æ£€ç´¢å™¨...")
base_retriever = vectorstore.as_retriever(search_kwargs={"k": 3})
print("âœ… æ£€ç´¢å™¨è®¾ç½®å®Œæˆ")

def decompose_query(complex_query, llm):
    """å°†å¤æ‚æŸ¥è¯¢åˆ†è§£ä¸ºå¤šä¸ªå­é—®é¢˜"""
    decompose_prompt = f"""
è¯·å°†ä»¥ä¸‹å¤æ‚æŸ¥è¯¢åˆ†è§£ä¸º3-4ä¸ªç›¸äº’ç‹¬ç«‹çš„å­é—®é¢˜ï¼Œæ¯ä¸ªå­é—®é¢˜éƒ½åº”è¯¥æ˜¯å¯ä»¥å•ç‹¬å›ç­”çš„ç®€å•é—®é¢˜ã€‚

å¤æ‚æŸ¥è¯¢: {complex_query}

åˆ†è§£è¦æ±‚:
1. æ¯ä¸ªå­é—®é¢˜éƒ½åº”è¯¥ç›¸äº’ç‹¬ç«‹
2. å­é—®é¢˜ç»„åˆèµ·æ¥åº”è¯¥èƒ½å®Œæ•´å›ç­”åŸå§‹æŸ¥è¯¢
3. æ¯ä¸ªå­é—®é¢˜åº”è¯¥ç®€æ´æ˜ç¡®
4. æ¯ä¸ªå­é—®é¢˜ä¸€è¡Œï¼Œä¸è¦ç¼–å·

åˆ†è§£åçš„å­é—®é¢˜:
    """
    
    response = llm.invoke(decompose_prompt)
    sub_queries = [line.strip() for line in response.content.strip().split('\n') if line.strip()]
    sub_queries = [q.lstrip('0123456789. ').rstrip('ï¼Ÿ?') + 'ï¼Ÿ' for q in sub_queries if q.strip()]
    
    return sub_queries

def query_decomposition_retrieve(complex_query, retriever, llm):
    """æŸ¥è¯¢åˆ†è§£æ£€ç´¢ï¼šå¤æ‚æŸ¥è¯¢åˆ†è§£ + å­é—®é¢˜æ£€ç´¢ + ç»“æœæ•´åˆ"""
    
    print("ğŸ”„ æ­¥éª¤1: åˆ†è§£å¤æ‚æŸ¥è¯¢...")
    sub_queries = decompose_query(complex_query, llm)
    print(f"   å°†å¤æ‚æŸ¥è¯¢åˆ†è§£ä¸º {len(sub_queries)} ä¸ªå­é—®é¢˜:")
    for i, sub_q in enumerate(sub_queries, 1):
        print(f"   [{i}] {sub_q}")
    
    print("\nğŸ” æ­¥éª¤2: å¯¹å„å­é—®é¢˜è¿›è¡Œæ£€ç´¢...")
    all_docs = []
    all_docs_content = set()  # ç”¨äºå»é‡
    
    for i, sub_query in enumerate(sub_queries, 1):
        print(f"\n   å­é—®é¢˜{i}: {sub_query}")
        docs = retriever.invoke(sub_query)
        print(f"   æ£€ç´¢åˆ° {len(docs)} ä¸ªç›¸å…³æ–‡æ¡£")
        
        # å»é‡æ·»åŠ æ–‡æ¡£
        new_docs = 0
        for doc in docs:
            if doc.page_content not in all_docs_content:
                all_docs.append(doc)
                all_docs_content.add(doc.page_content)
                new_docs += 1
        print(f"   æ–°å¢ {new_docs} ä¸ªç‹¬ç‰¹æ–‡æ¡£")
    
    print(f"\nğŸ“Š æ­¥éª¤3: æ•´åˆæ£€ç´¢ç»“æœ...")
    print(f"   æ€»å…±æ”¶é›†åˆ° {len(all_docs)} ä¸ªç›¸å…³æ–‡æ¡£")
    print(f"   æ¶µç›–äº† {len(sub_queries)} ä¸ªä¸åŒç»´åº¦çš„ä¿¡æ¯")
    
    return all_docs[:5]  # è¿”å›å‰5ä¸ªæœ€ç›¸å…³çš„æ–‡æ¡£

# è®¾è®¡ä¸€ä¸ªå¤æ‚æŸ¥è¯¢æ¡ˆä¾‹ï¼Œä½“ç°åˆ†è§£çš„ä»·å€¼
complex_query = "ç³–å°¿ç—…æ‚£è€…åº”è¯¥å¦‚ä½•æ§åˆ¶è¡€ç³–æ°´å¹³ï¼Œé¢„é˜²å“ªäº›å¹¶å‘ç—‡ï¼Œä»¥åŠæ—¥å¸¸é¥®é£Ÿéœ€è¦æ³¨æ„ä»€ä¹ˆï¼Ÿ"

print(f"\nğŸ” å¤æ‚æŸ¥è¯¢: ã€Œ{complex_query}ã€")
print("ğŸš€ å¼€å§‹æŸ¥è¯¢åˆ†è§£æ£€ç´¢...")

# ä½¿ç”¨æŸ¥è¯¢åˆ†è§£è¿›è¡Œæ£€ç´¢
docs = query_decomposition_retrieve(complex_query, base_retriever, llm)

print(f"\nğŸ“„ æŸ¥è¯¢åˆ†è§£æ£€ç´¢ç»“æœ:")
if isinstance(docs, list):
    for i, doc in enumerate(docs, 1):
        content = doc.page_content if hasattr(doc, "page_content") else str(doc)
        preview = content.replace('\n', ' ').strip()
        if len(preview) > 150:
            preview = preview[:150] + "..."
        print(f"[{i}] {preview}")
else:
    print("âŒ æœªæ‰¾åˆ°ç›¸å…³æ–‡æ¡£")

print(f"\nâœ… æŸ¥è¯¢åˆ†è§£æ£€ç´¢å®Œæˆï¼ä» {len(docs) if isinstance(docs, list) else 0} ä¸ªæ–‡æ¡£ä¸­è·å¾—å…¨é¢ä¿¡æ¯")

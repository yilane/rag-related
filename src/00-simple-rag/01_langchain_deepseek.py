"""
LangChain RAGæ™ºèƒ½é—®ç­”ç³»ç»Ÿ - DeepSeekç‰ˆæœ¬
ä½¿ç”¨ä¼ ç»Ÿçš„LangChainé“¾å¼è°ƒç”¨å®ç°æ£€ç´¢å¢å¼ºç”Ÿæˆ(RAG)
åŒ…å«æ–‡æ¡£åŠ è½½ã€å‘é‡åŒ–ã€æ£€ç´¢ã€ç”Ÿæˆç­‰å®Œæ•´æµç¨‹
"""
# ç¬¬ä¸€æ­¥ï¼šç´¢å¼•é˜¶æ®µ

# 1. åŠ è½½æ–‡æ¡£
# å¯¼å…¥å¿…è¦çš„æ¨¡å—å’Œç¯å¢ƒå˜é‡é…ç½®
import os
from dotenv import load_dotenv

# åŠ è½½.envæ–‡ä»¶ä¸­çš„ç¯å¢ƒå˜é‡ï¼ŒåŒ…æ‹¬APIå¯†é’¥ç­‰æ•æ„Ÿä¿¡æ¯
load_dotenv()

# å¯¼å…¥ç½‘é¡µæ–‡æ¡£åŠ è½½å™¨ï¼Œç”¨äºä»ç½‘é¡µçˆ¬å–å†…å®¹
from langchain_community.document_loaders import WebBaseLoader

# åˆ›å»ºç½‘é¡µåŠ è½½å™¨å®ä¾‹ï¼ŒæŒ‡å®šè¦çˆ¬å–çš„URL
loader = WebBaseLoader(web_paths=("https://zh.wikipedia.org/wiki/æ·±åº¦æ±‚ç´¢",))  # æ·±åº¦æ±‚ç´¢çš„ç»´åŸºç™¾ç§‘é¡µé¢
docs = loader.load()  # æ‰§è¡ŒåŠ è½½æ“ä½œï¼Œè¿”å›Documentå¯¹è±¡åˆ—è¡¨

# 2. æ–‡æœ¬åˆ†å—
# å¯¼å…¥é€’å½’å­—ç¬¦æ–‡æœ¬åˆ†å‰²å™¨ï¼Œç”¨äºå°†é•¿æ–‡æœ¬åˆ‡åˆ†æˆå°å—
from langchain_text_splitters import RecursiveCharacterTextSplitter

# åˆ›å»ºæ–‡æœ¬åˆ†å‰²å™¨å®ä¾‹
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=1000,  # æ¯ä¸ªæ–‡æœ¬å—çš„æœ€å¤§å­—ç¬¦æ•°ï¼ˆå¹³è¡¡å†…å®¹å®Œæ•´æ€§å’Œå¤„ç†æ•ˆç‡ï¼‰
    chunk_overlap=200,  # ç›¸é‚»æ–‡æœ¬å—ä¹‹é—´çš„é‡å å­—ç¬¦æ•°ï¼ˆç¡®ä¿ä¿¡æ¯ä¸ä¸¢å¤±ï¼‰
)
# å°†åŠ è½½çš„æ–‡æœ¬åˆ†å‰²æˆå¤šä¸ªå°å—ï¼Œä¾¿äºå‘é‡åŒ–å’Œæ£€ç´¢
all_splits = text_splitter.split_documents(docs)

# 3. ä¿¡æ¯åµŒå…¥
# å¯¼å…¥HuggingFaceåµŒå…¥æ¨¡å‹æ¥å£
from langchain_huggingface import HuggingFaceEmbeddings

# åˆå§‹åŒ–ä¸­æ–‡åµŒå…¥æ¨¡å‹
# ä½¿ç”¨BAAI/bge-small-zhæ¨¡å‹ï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“é—¨é’ˆå¯¹ä¸­æ–‡ä¼˜åŒ–çš„åµŒå…¥æ¨¡å‹
# model_kwargsæŒ‡å®šä½¿ç”¨CPUè¿›è¡Œè®¡ç®—ï¼Œé€‚åˆæ²¡æœ‰GPUçš„ç¯å¢ƒ
# encode_kwargsä¸­çš„normalize_embeddings=Trueç¡®ä¿åµŒå…¥å‘é‡è¢«å½’ä¸€åŒ–ï¼Œæé«˜æ£€ç´¢ç²¾åº¦
embeddings = HuggingFaceEmbeddings(
    model_name="BAAI/bge-small-zh",  # ä¸­æ–‡åµŒå…¥æ¨¡å‹ï¼Œä½“ç§¯å°ä½†æ•ˆæœè‰¯å¥½
    model_kwargs={"device": "cpu"},   # ä½¿ç”¨CPUè®¾å¤‡è¿›è¡Œè®¡ç®—ï¼ˆå¯æ”¹ä¸º"cuda"ä½¿ç”¨GPUï¼‰
    encode_kwargs={"normalize_embeddings": True},  # å¯ç”¨å‘é‡å½’ä¸€åŒ–ï¼Œæå‡ç›¸ä¼¼åº¦è®¡ç®—å‡†ç¡®æ€§
)

# 4. å‘é‡å­˜å‚¨
# å¯¼å…¥å†…å­˜å‘é‡å­˜å‚¨ï¼Œç”¨äºå­˜å‚¨å’Œæ£€ç´¢æ–‡æ¡£å‘é‡
from langchain_core.vectorstores import InMemoryVectorStore

# åˆ›å»ºå‘é‡å­˜å‚¨å®ä¾‹ï¼Œä¼ å…¥åµŒå…¥æ¨¡å‹
vector_store = InMemoryVectorStore(embeddings)
# å°†åˆ†å‰²åçš„æ–‡æ¡£æ·»åŠ åˆ°å‘é‡å­˜å‚¨ä¸­
# è¿™ä¸€æ­¥ä¼šå°†æ‰€æœ‰æ–‡æ¡£å—è½¬æ¢ä¸ºå‘é‡å¹¶å­˜å‚¨åœ¨å†…å­˜ä¸­ï¼Œç”¨äºåç»­çš„ç›¸ä¼¼æ€§æœç´¢
vector_store.add_documents(all_splits)

# ç¬¬äºŒæ­¥ï¼šæ£€ç´¢é˜¶æ®µ

# 5. æ„å»ºç”¨æˆ·æŸ¥è¯¢
# å®šä¹‰è¦è¯¢é—®çš„é—®é¢˜ï¼ˆè¿™é‡Œå¯ä»¥æ”¹ä¸ºåŠ¨æ€è¾“å…¥ï¼‰
question = "DeepSeekæœ‰å“ªäº›æ ¸å¿ƒæŠ€æœ¯ï¼Ÿ"

# 6. åœ¨å‘é‡å­˜å‚¨ä¸­æœç´¢ç›¸å…³æ–‡æ¡£ï¼Œå¹¶å‡†å¤‡ä¸Šä¸‹æ–‡å†…å®¹
# ä½¿ç”¨ä½™å¼¦ç›¸ä¼¼åº¦æœç´¢æœ€ç›¸å…³çš„æ–‡æ¡£å—
retrieved_docs = vector_store.similarity_search(question, k=3)  # k=3è¡¨ç¤ºæ£€ç´¢å‰3ä¸ªæœ€ç›¸å…³çš„æ–‡æ¡£å—
# å°†æ£€ç´¢åˆ°çš„æ–‡æ¡£å†…å®¹æ‹¼æ¥æˆä¸€ä¸ªå­—ç¬¦ä¸²ï¼Œä½œä¸ºå¤§æ¨¡å‹çš„ä¸Šä¸‹æ–‡
docs_content = "\n\n".join(doc.page_content for doc in retrieved_docs)

# 7. æ„å»ºæç¤ºæ¨¡æ¿
# å¯¼å…¥èŠå¤©æç¤ºæ¨¡æ¿ï¼Œç”¨äºæ ¼å¼åŒ–ç”¨æˆ·é—®é¢˜å’Œæ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡
from langchain_core.prompts import ChatPromptTemplate

# åˆ›å»ºæç¤ºæ¨¡æ¿ï¼Œå®šä¹‰å¤§æ¨¡å‹çš„è§’è‰²å’Œä»»åŠ¡
prompt = ChatPromptTemplate.from_template(
    """åŸºäºä»¥ä¸‹ä¸Šä¸‹æ–‡ï¼Œè¯·è¯¦ç»†å›ç­”é—®é¢˜ã€‚å¦‚æœä¸Šä¸‹æ–‡ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œ
è¯·è¯´"æˆ‘æ— æ³•ä»æä¾›çš„ä¸Šä¸‹æ–‡ä¸­æ‰¾åˆ°ç›¸å…³ä¿¡æ¯"ã€‚

ä¸Šä¸‹æ–‡: {context}

é—®é¢˜: {question}

å›ç­”:"""
)

# ç¬¬ä¸‰æ­¥ï¼šç”Ÿæˆé˜¶æ®µ

# 8. ä½¿ç”¨å¤§è¯­è¨€æ¨¡å‹ç”Ÿæˆç­”æ¡ˆ
# å¯¼å…¥DeepSeekèŠå¤©æ¨¡å‹
from langchain_deepseek import ChatDeepSeek  # éœ€è¦å®‰è£…: pip install langchain-deepseek

# åˆå§‹åŒ–DeepSeekèŠå¤©æ¨¡å‹
llm = ChatDeepSeek(
    model="deepseek-chat",  # DeepSeek API æ”¯æŒçš„èŠå¤©æ¨¡å‹åç§°
    temperature=0.7,  # æ§åˆ¶è¾“å‡ºçš„éšæœºæ€§ï¼ˆ0-1ä¹‹é—´ï¼Œè¶Šé«˜è¶Šéšæœºï¼Œè¶Šä½è¶Šç¡®å®šï¼‰
    max_tokens=2048,  # æœ€å¤§è¾“å‡ºtokenæ•°é‡ï¼Œæ§åˆ¶å›ç­”é•¿åº¦
    api_key=os.getenv("DEEPSEEK_API_KEY"),  # ä»ç¯å¢ƒå˜é‡åŠ è½½APIå¯†é’¥
)

# ä½¿ç”¨æç¤ºæ¨¡æ¿æ ¼å¼åŒ–é—®é¢˜å’Œä¸Šä¸‹æ–‡ï¼Œç„¶åè°ƒç”¨å¤§æ¨¡å‹ç”Ÿæˆç­”æ¡ˆ
formatted_prompt = prompt.format(question=question, context=docs_content)
answer = llm.invoke(formatted_prompt)

# æ ¼å¼åŒ–è¾“å‡ºç­”æ¡ˆ
print("=" * 80)
print("ğŸ¤– LangChain RAG æ™ºèƒ½é—®ç­”ç³»ç»Ÿ (DeepSeekç‰ˆæœ¬)")
print("=" * 80)
print(f"ğŸ“ é—®é¢˜: {question}")
print("-" * 80)
print("ğŸ’¡ ç­”æ¡ˆ:")
# å®‰å…¨åœ°æå–ç­”æ¡ˆå†…å®¹ï¼Œå…¼å®¹ä¸åŒçš„è¿”å›æ ¼å¼
print(answer.content if hasattr(answer, "content") else str(answer))
print("-" * 80)
print("ğŸ“š å‚è€ƒæ–‡æ¡£æ•°é‡:", len(retrieved_docs))
print("ğŸ” æ£€ç´¢åˆ°çš„ç›¸å…³ç‰‡æ®µ:")

# é€ä¸ªæ˜¾ç¤ºæ£€ç´¢åˆ°çš„æ–‡æ¡£ç‰‡æ®µï¼Œä¾¿äºç”¨æˆ·äº†è§£ç­”æ¡ˆæ¥æº
for i, doc in enumerate(retrieved_docs, 1):
    print(f"\nç‰‡æ®µ {i}:")
    # æˆªå–å‰200ä¸ªå­—ç¬¦æ˜¾ç¤ºï¼Œé¿å…è¾“å‡ºè¿‡é•¿
    content_preview = doc.page_content[:200]
    if len(doc.page_content) > 200:
        content_preview += "..."
    print(f"  å†…å®¹: {content_preview}")
    
    # å¦‚æœæ–‡æ¡£æœ‰å…ƒæ•°æ®ï¼ˆå¦‚æ¥æºURLï¼‰ï¼Œåˆ™æ˜¾ç¤º
    if hasattr(doc, "metadata") and doc.metadata:
        print(f"  æ¥æº: {doc.metadata}")

print("=" * 80)

# -*- coding: utf-8 -*-
"""
åŒ»å­¦å‘½åå®ä½“è¯†åˆ«(NER)æœåŠ¡æ¨¡å—
åŸºäºä¸­æ–‡åŒ»å­¦é¢„è®­ç»ƒæ¨¡å‹è¿›è¡Œå®ä½“è¯†åˆ«å’Œåˆ†ç±»
"""

import re
import logging
from typing import List, Dict, Any
import torch

from config import NER_CONFIG

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class MedicalNERService:
    """åŒ»å­¦å‘½åå®ä½“è¯†åˆ«æœåŠ¡ç±»"""
    
    def __init__(self, model_name: str = None):
        self.model_name = model_name or NER_CONFIG['model_name']
        self.confidence_threshold = NER_CONFIG['confidence_threshold']
        self.ner_pipeline = None
        self._load_model()
    
    def _load_model(self):
        """åŠ è½½NERæ¨¡å‹"""
        try:
            logger.info(f"æ­£åœ¨åŠ è½½åŒ»å­¦NERæ¨¡å‹: {self.model_name}")
            
            # å°è¯•åŠ è½½é¢„è®­ç»ƒæ¨¡å‹
            from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline
            
            tokenizer = AutoTokenizer.from_pretrained(self.model_name)
            model = AutoModelForTokenClassification.from_pretrained(self.model_name)
            
            self.ner_pipeline = pipeline(
                "ner",
                model=model,
                tokenizer=tokenizer,
                aggregation_strategy="simple",
                device=0 if torch.cuda.is_available() else -1
            )
            
            logger.info("åŒ»å­¦NERæ¨¡å‹åŠ è½½å®Œæˆ")
            
        except Exception as e:
            logger.error(f"åŠ è½½NERæ¨¡å‹å¤±è´¥: {e}")
            logger.info("ä½¿ç”¨å¤‡ç”¨NERæ–¹æ¡ˆ...")
            self._load_backup_ner()
    
    def _load_backup_ner(self):
        """å¤‡ç”¨NERæ–¹æ¡ˆï¼šåŸºäºè§„åˆ™çš„å®ä½“è¯†åˆ«"""
        logger.info("ä½¿ç”¨åŸºäºè§„åˆ™çš„å¤‡ç”¨NERè¯†åˆ«å™¨")
        self.ner_pipeline = None
        
        # å®šä¹‰åŒ»å­¦å®ä½“å…³é”®è¯è¯å…¸
        self.medical_keywords = {
            'DISEASE': [
                'å¿ƒè‚Œæ¢—æ­»', 'é«˜è¡€å‹', 'ç³–å°¿ç—…', 'è‚ºç‚', 'å“®å–˜', 'è‚ç‚', 
                'èƒƒç‚', 'è‚¾ç‚', 'å…³èŠ‚ç‚', 'è‚¿ç˜¤', 'ç™Œç—‡', 'ç™½è¡€ç—…',
                'è„‘æ¢—', 'ä¸­é£', 'éª¨æŠ˜', 'æ„Ÿå†’', 'å‘çƒ§', 'å’³å—½',
                'å¤´ç—›', 'è…¹ç—›', 'èƒ¸ç—›', 'å¿ƒæ‚¸', 'çœ©æ™•', 'å¤±çœ '
            ],
            'SYMPTOM': [
                'ç–¼ç—›', 'å‘çƒ­', 'å’³å—½', 'æ°”å–˜', 'ä¹åŠ›', 'æ¶å¿ƒ', 'å‘•å',
                'è…¹æ³»', 'ä¾¿ç§˜', 'å¤´æ™•', 'å¤´ç—›', 'èƒ¸é—·', 'å¿ƒæ‚¸', 'å¤±çœ ',
                'é£Ÿæ¬²ä¸æŒ¯', 'ä½“é‡ä¸‹é™', 'å‡ºè¡€', 'è‚¿èƒ€', 'ç˜™ç—’', 'çš®ç–¹'
            ],
            'BODY_PART': [
                'å¿ƒè„', 'è‚º', 'è‚è„', 'è‚¾è„', 'å¤§è„‘', 'èƒƒ', 'è‚ é“',
                'è„Šæ¤', 'å…³èŠ‚', 'è¡€ç®¡', 'ç¥ç»', 'çš®è‚¤', 'çœ¼ç›', 'è€³æœµ',
                'é¼»å­', 'å–‰å’™', 'æ‰‹', 'è„š', 'èƒ¸éƒ¨', 'è…¹éƒ¨', 'èƒŒéƒ¨'
            ],
            'EXAMINATION': [
                'CT', 'MRI', 'Xå…‰', 'Bè¶…', 'å¿ƒç”µå›¾', 'è¡€å¸¸è§„', 'å°¿å¸¸è§„',
                'è‚åŠŸèƒ½', 'è‚¾åŠŸèƒ½', 'è¡€ç³–', 'è¡€å‹', 'ä½“æ¸©', 'è„‰æ'
            ]
        }
    
    def extract_entities(self, text: str) -> List[Dict[str, Any]]:
        """ä»æ–‡æœ¬ä¸­æå–åŒ»å­¦å®ä½“"""
        try:
            if not text or not text.strip():
                return []
            
            # é¢„å¤„ç†æ–‡æœ¬
            cleaned_text = self._preprocess_text(text)
            
            if self.ner_pipeline is not None:
                # ä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹è¿›è¡ŒNER
                return self._extract_with_model(cleaned_text)
            else:
                # ä½¿ç”¨å¤‡ç”¨è§„åˆ™æ–¹æ³•
                return self._extract_with_rules(cleaned_text)
                
        except Exception as e:
            logger.error(f"å®ä½“æå–å¤±è´¥: {e}")
            return []
    
    def _preprocess_text(self, text: str) -> str:
        """æ–‡æœ¬é¢„å¤„ç†"""
        # å»é™¤å¤šä½™ç©ºæ ¼å’Œç‰¹æ®Šå­—ç¬¦
        text = re.sub(r'\s+', ' ', text)
        text = re.sub(r'[^\u4e00-\u9fa5a-zA-Z0-9\sï¼Œã€‚ï¼›ï¼šï¼ï¼Ÿ]', '', text)
        return text.strip()
    
    def _extract_with_model(self, text: str) -> List[Dict[str, Any]]:
        """ä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹æå–å®ä½“"""
        try:
            # è°ƒç”¨NER pipeline
            ner_results = self.ner_pipeline(text)
            
            entities = []
            for result in ner_results:
                if result['score'] >= self.confidence_threshold:
                    entity = {
                        'text': result['word'],
                        'label': result['entity_group'],
                        'confidence': round(result['score'], 3),
                        'start': result['start'],
                        'end': result['end']
                    }
                    entities.append(entity)
            
            # å»é‡å’Œåˆå¹¶ç›¸é‚»å®ä½“
            entities = self._merge_adjacent_entities(entities)
            
            return entities
            
        except Exception as e:
            logger.error(f"æ¨¡å‹æå–å®ä½“å¤±è´¥: {e}")
            return self._extract_with_rules(text)
    
    def _extract_with_rules(self, text: str) -> List[Dict[str, Any]]:
        """ä½¿ç”¨è§„åˆ™æ–¹æ³•æå–å®ä½“"""
        entities = []
        
        for label, keywords in self.medical_keywords.items():
            for keyword in keywords:
                # æŸ¥æ‰¾å…³é”®è¯åœ¨æ–‡æœ¬ä¸­çš„ä½ç½®
                for match in re.finditer(keyword, text):
                    entity = {
                        'text': keyword,
                        'label': label,
                        'confidence': 0.8,  # è§„åˆ™æ–¹æ³•ç»™å®šé»˜è®¤ç½®ä¿¡åº¦
                        'start': match.start(),
                        'end': match.end()
                    }
                    entities.append(entity)
        
        # æŒ‰ä½ç½®æ’åºå¹¶å»é‡
        entities = sorted(entities, key=lambda x: x['start'])
        entities = self._remove_overlapping_entities(entities)
        
        return entities
    
    def _merge_adjacent_entities(self, entities: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """åˆå¹¶ç›¸é‚»çš„åŒç±»å‹å®ä½“"""
        if not entities:
            return entities
        
        merged = []
        current = entities[0]
        
        for next_entity in entities[1:]:
            # å¦‚æœæ˜¯ç›¸é‚»çš„åŒç±»å‹å®ä½“ï¼Œåˆ™åˆå¹¶
            if (current['label'] == next_entity['label'] and 
                next_entity['start'] <= current['end'] + 2):
                current['text'] += next_entity['text']
                current['end'] = next_entity['end']
                current['confidence'] = max(current['confidence'], next_entity['confidence'])
            else:
                merged.append(current)
                current = next_entity
        
        merged.append(current)
        return merged
    
    def _remove_overlapping_entities(self, entities: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """ç§»é™¤é‡å çš„å®ä½“ï¼Œä¿ç•™ç½®ä¿¡åº¦æ›´é«˜çš„"""
        if not entities:
            return entities
        
        # æŒ‰ç½®ä¿¡åº¦é™åºæ’åº
        entities = sorted(entities, key=lambda x: x['confidence'], reverse=True)
        
        filtered = []
        for entity in entities:
            # æ£€æŸ¥æ˜¯å¦ä¸å·²é€‰æ‹©çš„å®ä½“é‡å 
            overlapped = False
            for selected in filtered:
                if (entity['start'] < selected['end'] and 
                    entity['end'] > selected['start']):
                    overlapped = True
                    break
            
            if not overlapped:
                filtered.append(entity)
        
        # æŒ‰ä½ç½®é‡æ–°æ’åº
        return sorted(filtered, key=lambda x: x['start'])
    
    def analyze_entities(self, entities: List[Dict[str, Any]]) -> Dict[str, Any]:
        """åˆ†ææå–çš„å®ä½“ç»Ÿè®¡ä¿¡æ¯"""
        if not entities:
            return {
                'total_entities': 0,
                'entity_types': {},
                'confidence_stats': {},
                'entities_by_type': {}
            }
        
        # ç»Ÿè®¡å„ç±»å‹å®ä½“æ•°é‡
        entity_types = {}
        entities_by_type = {}
        confidences = [e['confidence'] for e in entities]
        
        for entity in entities:
            label = entity['label']
            entity_types[label] = entity_types.get(label, 0) + 1
            
            if label not in entities_by_type:
                entities_by_type[label] = []
            entities_by_type[label].append(entity)
        
        return {
            'total_entities': len(entities),
            'entity_types': entity_types,
            'confidence_stats': {
                'mean': round(sum(confidences) / len(confidences), 3),
                'max': round(max(confidences), 3),
                'min': round(min(confidences), 3)
            },
            'entities_by_type': entities_by_type
        }
    
    def highlight_entities(self, text: str, entities: List[Dict[str, Any]]) -> str:
        """åœ¨åŸæ–‡ä¸­é«˜äº®æ˜¾ç¤ºè¯†åˆ«çš„å®ä½“"""
        if not entities:
            return text
        
        # æŒ‰ä½ç½®å€’åºæ’åºï¼Œé¿å…æ’å…¥æ ‡è®°æ—¶ä½ç½®åç§»
        entities = sorted(entities, key=lambda x: x['start'], reverse=True)
        
        highlighted_text = text
        color_map = {
            'DISEASE': '#FF6B6B',
            'SYMPTOM': '#4ECDC4', 
            'BODY_PART': '#45B7D1',
            'EXAMINATION': '#96CEB4',
            'DRUG': '#FFEAA7',
            'TREATMENT': '#DDA0DD'
        }
        
        for entity in entities:
            color = color_map.get(entity['label'], '#95A5A6')
            start, end = entity['start'], entity['end']
            original_text = highlighted_text[start:end]
            
            confidence_str = entity['confidence']
            highlighted = f'<span style="background-color: {color}; padding: 2px 4px; border-radius: 3px; color: white; font-weight: bold;" title="{entity["label"]} (ç½®ä¿¡åº¦: {confidence_str})">{original_text}</span>'
            
            highlighted_text = highlighted_text[:start] + highlighted + highlighted_text[end:]
        
        return highlighted_text
    
    def cleanup_resources(self) -> bool:
        """æ¸…ç†NERæ¨¡å‹èµ„æº"""
        try:
            logger.info("æ­£åœ¨æ¸…ç†NERæ¨¡å‹èµ„æº...")
            
            # æ¸…ç†æ¨¡å‹å¼•ç”¨
            if hasattr(self, 'ner_pipeline') and self.ner_pipeline is not None:
                # å¦‚æœæ˜¯Transformers pipelineï¼Œå°è¯•æ¸…ç†
                if hasattr(self.ner_pipeline, 'model'):
                    del self.ner_pipeline.model
                if hasattr(self.ner_pipeline, 'tokenizer'):
                    del self.ner_pipeline.tokenizer
                del self.ner_pipeline
                self.ner_pipeline = None
            
            # æ¸…ç†å…³é”®è¯è¯å…¸ï¼ˆå¦‚æœä½¿ç”¨å¤‡ç”¨æ–¹æ¡ˆï¼‰
            if hasattr(self, 'medical_keywords'):
                self.medical_keywords = None
            
            # å°è¯•æ¸…ç†CUDAç¼“å­˜
            try:
                import torch
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()
                    logger.info("CUDAç¼“å­˜å·²æ¸…ç†")
            except ImportError:
                pass
            
            logger.info("NERæ¨¡å‹èµ„æºæ¸…ç†å®Œæˆ")
            return True
            
        except Exception as e:
            logger.error(f"æ¸…ç†NERèµ„æºå¤±è´¥: {e}")
            return False
    
    def __del__(self):
        """ææ„å‡½æ•°ï¼šç¡®ä¿èµ„æºè¢«æ¸…ç†"""
        try:
            self.cleanup_resources()
        except:
            pass


def main():
    """æµ‹è¯•åŒ»å­¦NERæœåŠ¡"""
    try:
        # åˆ›å»ºNERæœåŠ¡å®ä¾‹
        ner_service = MedicalNERService()
        
        # æµ‹è¯•æ–‡æœ¬
        test_texts = [
            "æ‚£è€…ä¸»è¯‰èƒ¸ç—›3å¤©ï¼Œä¼´æœ‰å‘¼å¸å›°éš¾å’Œå¿ƒæ‚¸ï¼Œæ—¢å¾€æœ‰é«˜è¡€å‹ç—…å²",
            "è¯Šæ–­ä¸ºæ€¥æ€§å¿ƒè‚Œæ¢—æ­»ï¼Œå»ºè®®ç«‹å³è¿›è¡Œå¿ƒç”µå›¾æ£€æŸ¥",
            "æ‚£è€…å‡ºç°å‘çƒ­ã€å’³å—½ç—‡çŠ¶ï¼Œä½“æ¸©38.5åº¦ï¼Œå»ºè®®èƒ¸éƒ¨CTæ£€æŸ¥"
        ]
        
        print("ğŸ” åŒ»å­¦NERå®ä½“è¯†åˆ«æµ‹è¯•")
        print("=" * 50)
        
        for i, text in enumerate(test_texts, 1):
            print(f"\nğŸ“ æµ‹è¯•æ–‡æœ¬ {i}: {text}")
            
            # æå–å®ä½“
            entities = ner_service.extract_entities(text)
            
            if entities:
                print(f"âœ… è¯†åˆ«åˆ° {len(entities)} ä¸ªå®ä½“:")
                for entity in entities:
                    print(f"  - {entity['text']} ({entity['label']}, ç½®ä¿¡åº¦: {entity['confidence']})")
                
                # åˆ†æç»Ÿè®¡
                stats = ner_service.analyze_entities(entities)
                print(f"ğŸ“Š å®ä½“ç»Ÿè®¡: {stats['entity_types']}")
                
            else:
                print("âŒ æœªè¯†åˆ«åˆ°ä»»ä½•å®ä½“")
            
            print("-" * 30)
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")


if __name__ == "__main__":
    main()
"""
LlamaIndex æ‘˜è¦ç´¢å¼• (Summary Indexing) ç¤ºä¾‹

æœ¬è„šæœ¬æ¼”ç¤ºå¦‚ä½•ä½¿ç”¨æ‘˜è¦ç´¢å¼•ç­–ç•¥æ¥æå‡RAGç³»ç»Ÿçš„æ£€ç´¢æ•ˆç‡å’Œå‡†ç¡®æ€§ã€‚

æ‘˜è¦ç´¢å¼•çš„æ ¸å¿ƒæ€æƒ³ï¼š
1. ç´¢å¼•é˜¶æ®µï¼šä¸ºæ¯ä¸ªæ–‡æ¡£å—åˆ›å»ºç²¾ç‚¼çš„æ‘˜è¦ï¼ŒåŒæ—¶ä¿ç•™åŸæ–‡å—ï¼Œå»ºç«‹æ‘˜è¦ä¸åŸæ–‡çš„é“¾æ¥å…³ç³»
2. æ£€ç´¢é˜¶æ®µï¼šå…ˆåœ¨æ‘˜è¦å±‚è¿›è¡Œæ£€ç´¢ï¼Œåˆ©ç”¨æ‘˜è¦çš„ç®€æ´æ€§å’Œè¯­ä¹‰é›†ä¸­æ€§å¿«é€Ÿå®šä½ç›¸å…³å†…å®¹
3. è·å–åŸæ–‡ï¼šé€šè¿‡é“¾æ¥è·å–å¯¹åº”çš„å®Œæ•´åŸæ–‡å—ï¼Œä¸ºç”Ÿæˆæä¾›ä¸°å¯Œçš„ä¸Šä¸‹æ–‡ä¿¡æ¯

ä¸»è¦ä¼˜åŠ¿ï¼š
- æå‡æ£€ç´¢æ•ˆç‡ï¼šæ‘˜è¦æ›´çŸ­ï¼Œè¯­ä¹‰æ›´é›†ä¸­ï¼Œæ£€ç´¢é€Ÿåº¦æ›´å¿«
- æé«˜ç›¸å…³æ€§ï¼šé¿å…åœ¨å†—é•¿çš„åŸæ–‡ä¸­"è¿·è·¯"ï¼Œæ›´ç²¾å‡†åœ°åŒ¹é…ç”¨æˆ·æŸ¥è¯¢
- ä¿æŒå®Œæ•´æ€§ï¼šç”Ÿæˆæ—¶ä»å¯è·å–å®Œæ•´çš„åŸæ–‡ä¸Šä¸‹æ–‡ï¼Œä¸ä¸¢å¤±ä¿¡æ¯
- é™ä½å™ªå£°ï¼šæ‘˜è¦è¿‡æ»¤æ‰æ— å…³ç»†èŠ‚ï¼Œå‡å°‘æ£€ç´¢ç»“æœä¸­çš„å™ªå£°ä¿¡æ¯
"""

from llama_index.core import SimpleDirectoryReader, VectorStoreIndex
from llama_index.core.node_parser import SentenceSplitter
from llama_index.core.schema import Document, BaseNode, TextNode
from llama_index.embeddings.huggingface import HuggingFaceEmbedding
from llama_index.llms.deepseek import DeepSeek
import uuid
from typing import List, Dict, Any
import os

# åˆå§‹åŒ–åµŒå…¥æ¨¡å‹
embed_model = HuggingFaceEmbedding(model_name="BAAI/bge-small-zh")

# åˆå§‹åŒ–LLMç”¨äºç”Ÿæˆæ‘˜è¦
# ä½¿ç”¨DeepSeek APIï¼Œå¦‚æœæ²¡æœ‰APIå¯†é’¥åˆ™ä½¿ç”¨ç®€å•çš„æ–‡æœ¬æˆªæ–­
try:
    llm = DeepSeek(
        model="deepseek-chat",
        api_key=os.getenv("DEEPSEEK_API_KEY"),
        temperature=0.1
    )
    use_llm = True
    print("âœ… æˆåŠŸé…ç½® DeepSeek API ç”¨äºæ‘˜è¦ç”Ÿæˆ")
except Exception as e:
    print(f"[æç¤º] æœªé…ç½®DeepSeek APIï¼Œå°†ä½¿ç”¨ç®€å•æ–‡æœ¬æˆªæ–­ä½œä¸ºæ‘˜è¦: {e}")
    llm = None
    use_llm = False

def print_nodes(title, nodes, show_metadata=True):
    """ç”¨äºæ¸…æ™°æ‰“å°å’Œå¯è§†åŒ–åˆ†å—ç»“æœçš„è¾…åŠ©å‡½æ•°"""
    print("\n" + "=" * 60)
    print(f"  {title}  ")
    print("=" * 60)
    print(f"æ€»å—æ•°ï¼š{len(nodes)}\n")
    
    for i, node in enumerate(nodes, 1):
        text = node.text.strip()
        summary = text[:80] + ("..." if len(text) > 80 else "")
        print(f"--- ç¬¬ {i} ä¸ªå— ---")
        print(f"èŠ‚ç‚¹ID: {node.node_id}")
        print(f"å†…å®¹é•¿åº¦ï¼š{len(text)} å­—")
        print(f"å†…å®¹æ‘˜è¦ï¼š{summary}")
        
        if show_metadata and hasattr(node, 'metadata') and node.metadata:
            print(f"å…ƒæ•°æ®é”®: {list(node.metadata.keys())}")
            
            # æ˜¾ç¤ºæ‘˜è¦ä¿¡æ¯
            if 'summary' in node.metadata:
                summary_text = node.metadata['summary']
                print(f"èŠ‚ç‚¹æ‘˜è¦: {summary_text}")
                print(f"æ‘˜è¦é•¿åº¦: {len(summary_text)} å­—")
                
            # æ˜¾ç¤ºåŸæ–‡é“¾æ¥ä¿¡æ¯
            if 'original_node_id' in node.metadata:
                print(f"åŸæ–‡èŠ‚ç‚¹ID: {node.metadata['original_node_id']}")
                
            # æ˜¾ç¤ºèŠ‚ç‚¹ç±»å‹
            if 'node_type' in node.metadata:
                print(f"èŠ‚ç‚¹ç±»å‹: {node.metadata['node_type']}")
                
        print(f"å®Œæ•´å†…å®¹ï¼š\n{text}")
        print("-" * 50)

def generate_summary(text: str, max_length: int = 100) -> str:
    """ä¸ºç»™å®šæ–‡æœ¬ç”Ÿæˆæ‘˜è¦"""
    if use_llm and llm is not None:
        prompt = f"""è¯·ä¸ºä»¥ä¸‹æ–‡æœ¬ç”Ÿæˆä¸€ä¸ªç®€æ´çš„æ‘˜è¦ï¼Œé•¿åº¦æ§åˆ¶åœ¨{max_length}å­—ä»¥å†…ã€‚
æ‘˜è¦åº”è¯¥ï¼š
1. å‡†ç¡®æ¦‚æ‹¬æ–‡æœ¬çš„ä¸»è¦å†…å®¹å’Œæ ¸å¿ƒè§‚ç‚¹
2. ä¿æŒè¯­ä¹‰è¿è´¯æ€§å’Œå¯è¯»æ€§
3. å»é™¤å†—ä½™ä¿¡æ¯å’Œç»†èŠ‚æè¿°
4. ä½¿ç”¨ç®€æ´æ˜äº†çš„è¯­è¨€

åŸæ–‡ï¼š
{text}

æ‘˜è¦ï¼š"""
        
        try:
            response = llm.complete(prompt)
            summary = response.text.strip()
            # ç¡®ä¿æ‘˜è¦é•¿åº¦ä¸è¶…è¿‡é™åˆ¶
            if len(summary) > max_length:
                summary = summary[:max_length] + "..."
            return summary
        except Exception as e:
            print(f"[è­¦å‘Š] æ‘˜è¦ç”Ÿæˆå¤±è´¥: {e}")
            # å¦‚æœLLMå¤±è´¥ï¼Œä½¿ç”¨ç®€å•çš„æˆªæ–­ä½œä¸ºå¤‡ç”¨æ–¹æ¡ˆ
            return text[:max_length] + ("..." if len(text) > max_length else "")
    else:
        # ä½¿ç”¨ç®€å•çš„æ–‡æœ¬æˆªæ–­å’Œå…³é”®å¥æå–ä½œä¸ºæ‘˜è¦
        sentences = text.split('ã€‚')
        if len(sentences) > 1:
            # å–å‰ä¸¤ä¸ªå¥å­ä½œä¸ºæ‘˜è¦
            summary = sentences[0] + 'ã€‚' + (sentences[1] + 'ã€‚' if len(sentences[1]) > 0 else '')
            if len(summary) > max_length:
                summary = summary[:max_length] + "..."
            return summary
        else:
            return text[:max_length] + ("..." if len(text) > max_length else "")

class SummaryIndexNodeParser:
    """æ‘˜è¦ç´¢å¼•èŠ‚ç‚¹è§£æå™¨"""
    
    def __init__(self, 
                 chunk_size: int = 500,
                 chunk_overlap: int = 50,
                 summary_max_length: int = 100):
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap
        self.summary_max_length = summary_max_length
        self.base_splitter = SentenceSplitter(
            chunk_size=chunk_size,
            chunk_overlap=chunk_overlap
        )
    
    def get_nodes_from_documents(self, documents: List[Document]) -> List[BaseNode]:
        """ä»æ–‡æ¡£ç”Ÿæˆæ‘˜è¦ç´¢å¼•èŠ‚ç‚¹"""
        # ç¬¬ä¸€æ­¥ï¼šä½¿ç”¨åŸºç¡€åˆ†å—å™¨åˆ›å»ºåŸæ–‡å—
        original_nodes = self.base_splitter.get_nodes_from_documents(documents)
        print(f"åŸæ–‡åˆ†å—å®Œæˆï¼Œå…±ç”Ÿæˆ {len(original_nodes)} ä¸ªåŸæ–‡å—")
        
        # ç¬¬äºŒæ­¥ï¼šä¸ºæ¯ä¸ªåŸæ–‡å—ç”Ÿæˆæ‘˜è¦èŠ‚ç‚¹
        summary_nodes = []
        node_mapping = {}  # ç”¨äºå­˜å‚¨æ‘˜è¦èŠ‚ç‚¹å’ŒåŸæ–‡èŠ‚ç‚¹çš„æ˜ å°„å…³ç³»
        
        for i, original_node in enumerate(original_nodes):
            print(f"æ­£åœ¨ç”Ÿæˆç¬¬ {i+1}/{len(original_nodes)} ä¸ªæ‘˜è¦...")
            
            # ç”Ÿæˆæ‘˜è¦
            summary_text = generate_summary(original_node.text, self.summary_max_length)
            
            # åˆ›å»ºæ‘˜è¦èŠ‚ç‚¹
            summary_node = TextNode(
                text=summary_text,
                id_=str(uuid.uuid4()),
                metadata={
                    'node_type': 'summary',
                    'original_node_id': original_node.node_id,
                    'summary': summary_text,
                    'original_length': len(original_node.text),
                    'summary_length': len(summary_text)
                }
            )
            
            # æ›´æ–°åŸæ–‡èŠ‚ç‚¹çš„å…ƒæ•°æ®
            original_node.metadata = original_node.metadata or {}
            original_node.metadata.update({
                'node_type': 'original',
                'summary_node_id': summary_node.node_id,
                'summary': summary_text,
                'original_length': len(original_node.text),
                'summary_length': len(summary_text)
            })
            
            summary_nodes.append(summary_node)
            node_mapping[summary_node.node_id] = original_node.node_id
        
        print(f"æ‘˜è¦ç”Ÿæˆå®Œæˆï¼Œå…±ç”Ÿæˆ {len(summary_nodes)} ä¸ªæ‘˜è¦èŠ‚ç‚¹")
        
        # ç¬¬ä¸‰æ­¥ï¼šè¿”å›æ‘˜è¦èŠ‚ç‚¹å’ŒåŸæ–‡èŠ‚ç‚¹ï¼Œä»¥åŠæ˜ å°„å…³ç³»
        return summary_nodes, original_nodes, node_mapping

def create_summary_index(summary_nodes: List[BaseNode], 
                        original_nodes: List[BaseNode],
                        node_mapping: Dict[str, str]) -> Dict[str, Any]:
    """åˆ›å»ºæ‘˜è¦ç´¢å¼•ç³»ç»Ÿ"""
    
    # åˆ›å»ºæ‘˜è¦èŠ‚ç‚¹çš„å‘é‡ç´¢å¼•
    summary_index = VectorStoreIndex(
        nodes=summary_nodes,
        embed_model=embed_model
    )
    
    # åˆ›å»ºåŸæ–‡èŠ‚ç‚¹çš„æ˜ å°„å­—å…¸ï¼ˆç”¨äºå¿«é€ŸæŸ¥æ‰¾ï¼‰
    original_nodes_dict = {node.node_id: node for node in original_nodes}
    
    # åˆ›å»ºæ‘˜è¦ç´¢å¼•ç³»ç»Ÿ
    summary_index_system = {
        'summary_index': summary_index,
        'original_nodes': original_nodes_dict,
        'node_mapping': node_mapping
    }
    
    return summary_index_system

def search_with_summary_index(query: str, 
                            summary_index_system: Dict[str, Any],
                            top_k: int = 3) -> List[BaseNode]:
    """ä½¿ç”¨æ‘˜è¦ç´¢å¼•è¿›è¡Œæ£€ç´¢"""
    
    # ç¬¬ä¸€æ­¥ï¼šåœ¨æ‘˜è¦å±‚è¿›è¡Œæ£€ç´¢
    print(f"\nğŸ” ç¬¬ä¸€æ­¥ï¼šåœ¨æ‘˜è¦å±‚æ£€ç´¢ç›¸å…³å†…å®¹...")
    print(f"æŸ¥è¯¢ï¼š{query}")
    
    summary_index = summary_index_system['summary_index']
    summary_retriever = summary_index.as_retriever(similarity_top_k=top_k)
    summary_results = summary_retriever.retrieve(query)
    
    print(f"æ‘˜è¦å±‚æ£€ç´¢ç»“æœï¼š")
    for i, result in enumerate(summary_results, 1):
        print(f"  {i}. [ç›¸ä¼¼åº¦: {result.score:.3f}] {result.text}")
    
    # ç¬¬äºŒæ­¥ï¼šé€šè¿‡æ˜ å°„è·å–å¯¹åº”çš„åŸæ–‡èŠ‚ç‚¹
    print(f"\nğŸ“„ ç¬¬äºŒæ­¥ï¼šè·å–å¯¹åº”çš„åŸæ–‡å†…å®¹...")
    original_nodes = []
    node_mapping = summary_index_system['node_mapping']
    original_nodes_dict = summary_index_system['original_nodes']
    
    for i, summary_result in enumerate(summary_results, 1):
        summary_node_id = summary_result.node.node_id
        if summary_node_id in node_mapping:
            original_node_id = node_mapping[summary_node_id]
            if original_node_id in original_nodes_dict:
                original_node = original_nodes_dict[original_node_id]
                original_nodes.append(original_node)
                print(f"  {i}. åŸæ–‡ç‰‡æ®µ (é•¿åº¦: {len(original_node.text)} å­—): {original_node.text[:100]}...")
            else:
                print(f"  {i}. âŒ æœªæ‰¾åˆ°åŸæ–‡èŠ‚ç‚¹ {original_node_id}")
        else:
            print(f"  {i}. âŒ æœªæ‰¾åˆ°æ‘˜è¦èŠ‚ç‚¹æ˜ å°„ {summary_node_id}")
    
    return original_nodes

def analyze_summary_effectiveness(summary_nodes: List[BaseNode], 
                                original_nodes: List[BaseNode]):
    """åˆ†ææ‘˜è¦ç´¢å¼•çš„æ•ˆæœ"""
    print("\n" + "=" * 60)
    print("æ‘˜è¦ç´¢å¼•æ•ˆæœåˆ†æ")
    print("=" * 60)
    
    # ç»Ÿè®¡åŸºæœ¬ä¿¡æ¯
    total_original_length = sum(len(node.text) for node in original_nodes)
    total_summary_length = sum(len(node.text) for node in summary_nodes)
    
    print(f"åŸæ–‡èŠ‚ç‚¹æ•°é‡: {len(original_nodes)}")
    print(f"æ‘˜è¦èŠ‚ç‚¹æ•°é‡: {len(summary_nodes)}")
    print(f"åŸæ–‡æ€»é•¿åº¦: {total_original_length} å­—")
    print(f"æ‘˜è¦æ€»é•¿åº¦: {total_summary_length} å­—")
    print(f"å‹ç¼©æ¯”ä¾‹: {total_summary_length / total_original_length:.2%}")
    
    # ç»Ÿè®¡é•¿åº¦åˆ†å¸ƒ
    original_lengths = [len(node.text) for node in original_nodes]
    summary_lengths = [len(node.text) for node in summary_nodes]
    
    print(f"\nåŸæ–‡å—é•¿åº¦ - å¹³å‡: {sum(original_lengths) / len(original_lengths):.1f} å­—")
    print(f"åŸæ–‡å—é•¿åº¦ - èŒƒå›´: {min(original_lengths)} - {max(original_lengths)} å­—")
    print(f"æ‘˜è¦å—é•¿åº¦ - å¹³å‡: {sum(summary_lengths) / len(summary_lengths):.1f} å­—")
    print(f"æ‘˜è¦å—é•¿åº¦ - èŒƒå›´: {min(summary_lengths)} - {max(summary_lengths)} å­—")
    
    # åˆ†æå‹ç¼©æ•ˆæœ
    compression_ratios = []
    for i, (original_node, summary_node) in enumerate(zip(original_nodes, summary_nodes)):
        ratio = len(summary_node.text) / len(original_node.text)
        compression_ratios.append(ratio)
    
    avg_compression = sum(compression_ratios) / len(compression_ratios)
    print(f"\nå¹³å‡å‹ç¼©æ¯”ä¾‹: {avg_compression:.2%}")
    print(f"æœ€é«˜å‹ç¼©æ¯”ä¾‹: {min(compression_ratios):.2%}")
    print(f"æœ€ä½å‹ç¼©æ¯”ä¾‹: {max(compression_ratios):.2%}")

# ä¸»æ¼”ç¤ºæµç¨‹
def main():
    print("=" * 60)
    print("æ‘˜è¦ç´¢å¼• (Summary Indexing) æ¼”ç¤º")
    print("=" * 60)
    
    # 1. è¯»å–ç¤ºä¾‹æ–‡æ¡£
    try:
        documents = SimpleDirectoryReader(
            input_files=["data/txt/ç³–å°¿ç—….txt"]
        ).load_data()
        print(f"âœ… æˆåŠŸè¯»å–æ–‡æ¡£ï¼Œå…± {len(documents)} ä¸ªæ–‡æ¡£")
        print(f"æ–‡æ¡£æ€»é•¿åº¦ï¼š{len(documents[0].text)} å­—")
    except Exception as e:
        print(f"[é”™è¯¯] ç³–å°¿ç—…æ–‡æ¡£è¯»å–å¤±è´¥: {e}")
        try:
            documents = SimpleDirectoryReader(
                input_files=["data/txt/Qwen3.txt"]
            ).load_data()
            print(f"âœ… å›é€€ä½¿ç”¨ Qwen3.txtï¼Œæ–‡æ¡£é•¿åº¦ï¼š{len(documents[0].text)} å­—")
        except Exception as e2:
            print(f"[é”™è¯¯] æ–‡æ¡£è¯»å–å¤±è´¥: {e2}")
            return
    
    # 2. åˆ›å»ºæ‘˜è¦ç´¢å¼•èŠ‚ç‚¹è§£æå™¨
    print("\nğŸ“ åˆ›å»ºæ‘˜è¦ç´¢å¼•èŠ‚ç‚¹è§£æå™¨...")
    summary_parser = SummaryIndexNodeParser(
        chunk_size=400,      # åŸæ–‡å—å¤§å°
        chunk_overlap=50,    # åŸæ–‡å—é‡å 
        summary_max_length=80  # æ‘˜è¦æœ€å¤§é•¿åº¦
    )
    
    # 3. ç”Ÿæˆæ‘˜è¦ç´¢å¼•èŠ‚ç‚¹
    print("\nğŸ”„ ç”Ÿæˆæ‘˜è¦ç´¢å¼•èŠ‚ç‚¹...")
    summary_nodes, original_nodes, node_mapping = summary_parser.get_nodes_from_documents(documents)
    
    # 4. å±•ç¤ºæ‘˜è¦èŠ‚ç‚¹
    print_nodes("æ‘˜è¦èŠ‚ç‚¹å±•ç¤º", summary_nodes[:3])
    
    # 5. å±•ç¤ºåŸæ–‡èŠ‚ç‚¹
    print_nodes("åŸæ–‡èŠ‚ç‚¹å±•ç¤º", original_nodes[:3])
    
    # 6. åˆ†ææ‘˜è¦æ•ˆæœ
    analyze_summary_effectiveness(summary_nodes, original_nodes)
    
    # 7. åˆ›å»ºæ‘˜è¦ç´¢å¼•ç³»ç»Ÿ
    print("\nğŸ—ï¸ åˆ›å»ºæ‘˜è¦ç´¢å¼•ç³»ç»Ÿ...")
    summary_index_system = create_summary_index(summary_nodes, original_nodes, node_mapping)
    print("âœ… æ‘˜è¦ç´¢å¼•ç³»ç»Ÿåˆ›å»ºå®Œæˆ")
    
    # 8. æ¼”ç¤ºæ‘˜è¦ç´¢å¼•æ£€ç´¢
    print("\n" + "=" * 60)
    print("æ‘˜è¦ç´¢å¼•æ£€ç´¢æ¼”ç¤º")
    print("=" * 60)
    
    # ç¤ºä¾‹æŸ¥è¯¢
    test_queries = [
        "ç³–å°¿ç—…çš„ç—‡çŠ¶æœ‰å“ªäº›ï¼Ÿ",
        "å¦‚ä½•é¢„é˜²ç³–å°¿ç—…ï¼Ÿ",
        "ç³–å°¿ç—…çš„æ²»ç–—æ–¹æ³•"
    ]
    
    for query in test_queries:
        print(f"\n{'='*40}")
        print(f"æŸ¥è¯¢: {query}")
        print('='*40)
        
        # ä½¿ç”¨æ‘˜è¦ç´¢å¼•æ£€ç´¢
        retrieved_nodes = search_with_summary_index(
            query, 
            summary_index_system, 
            top_k=2
        )
        
        print(f"\nâœ… æœ€ç»ˆæ£€ç´¢ç»“æœï¼š{len(retrieved_nodes)} ä¸ªåŸæ–‡ç‰‡æ®µ")
        for i, node in enumerate(retrieved_nodes, 1):
            print(f"\nç‰‡æ®µ {i}:")
            print(f"é•¿åº¦: {len(node.text)} å­—")
            print(f"å†…å®¹: {node.text[:200]}...")
            if 'summary' in node.metadata:
                print(f"æ‘˜è¦: {node.metadata['summary']}")
    
    # 9. å¯¹æ¯”ä¼ ç»Ÿæ£€ç´¢
    print("\n" + "=" * 60)
    print("ä¼ ç»Ÿæ£€ç´¢ vs æ‘˜è¦ç´¢å¼•æ£€ç´¢å¯¹æ¯”")
    print("=" * 60)
    
    # åˆ›å»ºä¼ ç»Ÿç´¢å¼•ç”¨äºå¯¹æ¯”
    traditional_index = VectorStoreIndex(
        nodes=original_nodes,
        embed_model=embed_model
    )
    
    query = "ç³–å°¿ç—…çš„ä¸»è¦ç—‡çŠ¶"
    print(f"å¯¹æ¯”æŸ¥è¯¢: {query}")
    
    # ä¼ ç»Ÿæ£€ç´¢
    print(f"\nğŸ“Š ä¼ ç»Ÿæ£€ç´¢ç»“æœ:")
    traditional_retriever = traditional_index.as_retriever(similarity_top_k=2)
    traditional_results = traditional_retriever.retrieve(query)
    for i, result in enumerate(traditional_results, 1):
        print(f"  {i}. [ç›¸ä¼¼åº¦: {result.score:.3f}] {result.text[:100]}...")
    
    # æ‘˜è¦ç´¢å¼•æ£€ç´¢
    print(f"\nğŸ¯ æ‘˜è¦ç´¢å¼•æ£€ç´¢ç»“æœ:")
    summary_results = search_with_summary_index(query, summary_index_system, top_k=2)
    for i, node in enumerate(summary_results, 1):
        print(f"  {i}. [é€šè¿‡æ‘˜è¦æ£€ç´¢] {node.text[:100]}...")

if __name__ == "__main__":
    main()
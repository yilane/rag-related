"""
LangGraph RAGæ™ºèƒ½é—®ç­”ç³»ç»Ÿ
ä½¿ç”¨LangGraphæ„å»ºæ£€ç´¢å¢å¼ºç”Ÿæˆ(RAG)å·¥ä½œæµ
åŒ…å«æ–‡æ¡£æ£€ç´¢å’Œç­”æ¡ˆç”Ÿæˆä¸¤ä¸ªæ­¥éª¤çš„å›¾çŠ¶æ‰§è¡Œæµç¨‹
"""

import os
from typing import List
from typing_extensions import TypedDict
from langchain_core.documents import Document

# 1. åŠ è½½æ–‡æ¡£
# ä½¿ç”¨WebBaseLoaderä»ç½‘é¡µåŠ è½½æ–‡æ¡£å†…å®¹
from langchain_community.document_loaders import WebBaseLoader

loader = WebBaseLoader(
    web_paths=("https://zh.wikipedia.org/wiki/æ·±åº¦æ±‚ç´¢",)  # æŒ‡å®šè¦çˆ¬å–çš„ç½‘é¡µURLåˆ—è¡¨
)
docs = loader.load()  # æ‰§è¡ŒåŠ è½½ï¼Œè¿”å›Documentå¯¹è±¡åˆ—è¡¨

# 2. æ–‡æ¡£åˆ†å—
# å°†é•¿æ–‡æ¡£åˆ‡åˆ†æˆè¾ƒå°çš„chunkï¼Œä¾¿äºå‘é‡åŒ–å’Œæ£€ç´¢
from langchain_text_splitters import RecursiveCharacterTextSplitter

text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=1000,      # æ¯ä¸ªæ–‡æœ¬å—çš„æœ€å¤§å­—ç¬¦æ•°
    chunk_overlap=200     # ç›¸é‚»æ–‡æœ¬å—ä¹‹é—´çš„é‡å å­—ç¬¦æ•°ï¼Œç¡®ä¿ä¿¡æ¯è¿è´¯æ€§
)
all_splits = text_splitter.split_documents(docs)  # å°†æ–‡æ¡£åˆ†å‰²æˆå¤šä¸ªchunk

# 3. è®¾ç½®åµŒå…¥æ¨¡å‹
# ä½¿ç”¨HuggingFaceçš„ä¸­æ–‡åµŒå…¥æ¨¡å‹å°†æ–‡æœ¬è½¬æ¢ä¸ºå‘é‡è¡¨ç¤º
from langchain_huggingface import HuggingFaceEmbeddings

embeddings = HuggingFaceEmbeddings(
    model_name="BAAI/bge-small-zh",                    # ä½¿ç”¨bge-small-zhä¸­æ–‡åµŒå…¥æ¨¡å‹
    model_kwargs={'device': 'cpu'},                    # æŒ‡å®šä½¿ç”¨CPUè¿è¡Œï¼ˆå¯æ”¹ä¸º'cuda'ä½¿ç”¨GPUï¼‰
    encode_kwargs={'normalize_embeddings': True}       # å¯¹åµŒå…¥å‘é‡è¿›è¡Œå½’ä¸€åŒ–å¤„ç†
)

# 4. åˆ›å»ºå‘é‡å­˜å‚¨
# ä½¿ç”¨å†…å­˜å‘é‡å­˜å‚¨æ¥å­˜å‚¨å’Œæ£€ç´¢æ–‡æ¡£å‘é‡
from langchain_core.vectorstores import InMemoryVectorStore

vector_store = InMemoryVectorStore(embeddings)  # åˆå§‹åŒ–å‘é‡å­˜å‚¨ï¼Œä¼ å…¥åµŒå…¥æ¨¡å‹
vector_store.add_documents(all_splits)          # å°†åˆ†å‰²åçš„æ–‡æ¡£æ·»åŠ åˆ°å‘é‡å­˜å‚¨ä¸­

# 5. å®šä¹‰RAGæç¤ºè¯
# ä»LangChain Hubæ‹‰å–é¢„å®šä¹‰çš„RAGæç¤ºè¯æ¨¡æ¿
from langchain import hub

prompt = hub.pull("rlm/rag-prompt")  # è·å–æ ‡å‡†çš„RAGæç¤ºè¯æ¨¡æ¿

# 6. å®šä¹‰åº”ç”¨çŠ¶æ€
# ä½¿ç”¨TypedDictå®šä¹‰LangGraphå·¥ä½œæµä¸­çš„çŠ¶æ€ç»“æ„
class State(TypedDict):
    """
    LangGraphçŠ¶æ€å®šä¹‰
    
    Attributes:
        question: ç”¨æˆ·è¾“å…¥çš„é—®é¢˜å­—ç¬¦ä¸²
        context: æ£€ç´¢åˆ°çš„ç›¸å…³æ–‡æ¡£åˆ—è¡¨
        answer: ç”Ÿæˆçš„ç­”æ¡ˆå­—ç¬¦ä¸²
    """
    question: str              # ç”¨æˆ·æå‡ºçš„é—®é¢˜
    context: List[Document]    # ä»å‘é‡å­˜å‚¨ä¸­æ£€ç´¢åˆ°çš„ç›¸å…³æ–‡æ¡£
    answer: str               # å¤§è¯­è¨€æ¨¡å‹ç”Ÿæˆçš„æœ€ç»ˆç­”æ¡ˆ

# 7. å®šä¹‰æ£€ç´¢æ­¥éª¤
def retrieve(state: State) -> dict:
    """
    æ£€ç´¢æ­¥éª¤ï¼šæ ¹æ®é—®é¢˜ä»å‘é‡å­˜å‚¨ä¸­æ£€ç´¢ç›¸å…³æ–‡æ¡£
    
    Args:
        state: å½“å‰çŠ¶æ€ï¼ŒåŒ…å«ç”¨æˆ·é—®é¢˜
        
    Returns:
        dict: åŒ…å«æ£€ç´¢åˆ°çš„æ–‡æ¡£çš„å­—å…¸ï¼Œé”®ä¸º"context"
    """
    # ä½¿ç”¨ç›¸ä¼¼åº¦æœç´¢æ£€ç´¢ä¸é—®é¢˜æœ€ç›¸å…³çš„æ–‡æ¡£
    retrieved_docs = vector_store.similarity_search(state["question"])
    return {"context": retrieved_docs}  # è¿”å›æ£€ç´¢ç»“æœï¼Œæ›´æ–°çŠ¶æ€ä¸­çš„contextå­—æ®µ

# 8. å®šä¹‰ç”Ÿæˆæ­¥éª¤
def generate(state: State) -> dict:
    """
    ç”Ÿæˆæ­¥éª¤ï¼šåŸºäºæ£€ç´¢åˆ°çš„æ–‡æ¡£å’Œé—®é¢˜ç”Ÿæˆç­”æ¡ˆ
    
    Args:
        state: å½“å‰çŠ¶æ€ï¼ŒåŒ…å«é—®é¢˜å’Œæ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡æ–‡æ¡£
        
    Returns:
        dict: åŒ…å«ç”Ÿæˆç­”æ¡ˆçš„å­—å…¸ï¼Œé”®ä¸º"answer"
    """
    # å¯¼å…¥DeepSeekèŠå¤©æ¨¡å‹
    from langchain_deepseek import ChatDeepSeek
    
    # åˆå§‹åŒ–DeepSeekå¤§è¯­è¨€æ¨¡å‹
    llm = ChatDeepSeek(
        model="deepseek-chat",                      # ä½¿ç”¨deepseek-chatæ¨¡å‹
        temperature=0.7,                            # æ§åˆ¶ç”Ÿæˆæ–‡æœ¬çš„éšæœºæ€§(0-1ï¼Œè¶Šé«˜è¶Šéšæœº)
        max_tokens=2048,                           # æœ€å¤§ç”Ÿæˆtokenæ•°é‡
        api_key=os.getenv("DEEPSEEK_API_KEY"),     # ä»ç¯å¢ƒå˜é‡è·å–APIå¯†é’¥
    )
    
    # å°†æ£€ç´¢åˆ°çš„æ–‡æ¡£å†…å®¹æ‹¼æ¥æˆä¸Šä¸‹æ–‡å­—ç¬¦ä¸²
    docs_content = "\n\n".join(doc.page_content for doc in state["context"])
    
    # ä½¿ç”¨æç¤ºè¯æ¨¡æ¿æ ¼å¼åŒ–é—®é¢˜å’Œä¸Šä¸‹æ–‡
    messages = prompt.invoke({
        "question": state["question"], 
        "context": docs_content
    })
    
    # è°ƒç”¨å¤§è¯­è¨€æ¨¡å‹ç”Ÿæˆç­”æ¡ˆ
    response = llm.invoke(messages)
    return {"answer": response.content}  # è¿”å›ç”Ÿæˆçš„ç­”æ¡ˆï¼Œæ›´æ–°çŠ¶æ€ä¸­çš„answerå­—æ®µ

# 9. æ„å»ºå’Œç¼–è¯‘åº”ç”¨
# ä½¿ç”¨LangGraphæ„å»ºåŒ…å«æ£€ç´¢å’Œç”Ÿæˆæ­¥éª¤çš„å·¥ä½œæµå›¾
from langgraph.graph import START, StateGraph

# åˆ›å»ºçŠ¶æ€å›¾å¹¶å®šä¹‰æ‰§è¡Œæµç¨‹
graph = (
    StateGraph(State)                          # åˆ›å»ºçŠ¶æ€å›¾ï¼ŒæŒ‡å®šçŠ¶æ€ç±»å‹
    .add_sequence([retrieve, generate])        # æ·»åŠ é¡ºåºæ‰§è¡Œçš„èŠ‚ç‚¹åºåˆ—ï¼šå…ˆæ£€ç´¢ï¼Œåç”Ÿæˆ
    .add_edge(START, "retrieve")              # æ·»åŠ ä»å¼€å§‹èŠ‚ç‚¹åˆ°æ£€ç´¢èŠ‚ç‚¹çš„è¾¹
    .compile()                                # ç¼–è¯‘å›¾ï¼Œç”Ÿæˆå¯æ‰§è¡Œçš„å·¥ä½œæµ
)

# 10. è¿è¡ŒæŸ¥è¯¢
# æ‰§è¡Œå®Œæ•´çš„RAGå·¥ä½œæµï¼šé—®é¢˜ -> æ£€ç´¢ -> ç”Ÿæˆ -> ç­”æ¡ˆ
question = "DeepSeekæœ‰å“ªäº›æ ¸å¿ƒæŠ€æœ¯ï¼Ÿ"      # å®šä¹‰è¦æŸ¥è¯¢çš„é—®é¢˜
response = graph.invoke({"question": question})  # è°ƒç”¨å›¾æ‰§è¡Œå™¨ï¼Œä¼ å…¥åˆå§‹çŠ¶æ€

# 11. æ ¼å¼åŒ–è¾“å‡ºç»“æœ
print("=" * 80)
print("ğŸ¤– LangGraph RAG æ™ºèƒ½é—®ç­”ç³»ç»Ÿ")
print("=" * 80)
print(f"ğŸ“ é—®é¢˜: {question}")
print("-" * 80)
print("ğŸ’¡ ç­”æ¡ˆ:")
print(response["answer"])
print("-" * 80)
print(f"ğŸ“š æ£€ç´¢åˆ°çš„æ–‡æ¡£æ•°é‡: {len(response.get('context', []))}")
print("=" * 80)
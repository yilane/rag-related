# 1. åŠ è½½æ–‡æ¡£
import os
from dotenv import load_dotenv

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

from langchain_community.document_loaders import WebBaseLoader

loader = WebBaseLoader(web_paths=("https://zh.wikipedia.org/wiki/æ·±åº¦æ±‚ç´¢",))
docs = loader.load()

# 2. æ–‡æ¡£åˆ†å—
from langchain_text_splitters import RecursiveCharacterTextSplitter

text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=1000,  # æ¯ä¸ªæ–‡æœ¬å—çš„æœ€å¤§å­—ç¬¦æ•°
    chunk_overlap=200,  # ç›¸é‚»æ–‡æœ¬å—ä¹‹é—´çš„é‡å å­—ç¬¦æ•°
)
all_splits = text_splitter.split_documents(docs)

# 3. è®¾ç½®åµŒå…¥æ¨¡å‹
from langchain_huggingface import HuggingFaceEmbeddings

# åˆå§‹åŒ–ä¸­æ–‡åµŒå…¥æ¨¡å‹
# ä½¿ç”¨BAAI/bge-small-zhæ¨¡å‹ï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“é—¨é’ˆå¯¹ä¸­æ–‡ä¼˜åŒ–çš„åµŒå…¥æ¨¡å‹
# model_kwargsæŒ‡å®šä½¿ç”¨CPUè¿›è¡Œè®¡ç®—ï¼Œé€‚åˆæ²¡æœ‰GPUçš„ç¯å¢ƒ
# encode_kwargsä¸­çš„normalize_embeddings=Trueç¡®ä¿åµŒå…¥å‘é‡è¢«å½’ä¸€åŒ–ï¼Œæé«˜æ£€ç´¢ç²¾åº¦
embeddings = HuggingFaceEmbeddings(
    model_name="BAAI/bge-small-zh",  # ä¸­æ–‡åµŒå…¥æ¨¡å‹ï¼Œä½“ç§¯å°ä½†æ•ˆæœè‰¯å¥½
    model_kwargs={"device": "cpu"},   # ä½¿ç”¨CPUè®¾å¤‡è¿›è¡Œè®¡ç®—
    encode_kwargs={"normalize_embeddings": True},  # å¯ç”¨å‘é‡å½’ä¸€åŒ–
)

# 4. åˆ›å»ºå‘é‡å­˜å‚¨
from langchain_core.vectorstores import InMemoryVectorStore

vector_store = InMemoryVectorStore(embeddings)
# å°†åˆ†å‰²åçš„æ–‡æ¡£æ·»åŠ åˆ°å‘é‡å­˜å‚¨ä¸­
# è¿™ä¸€æ­¥ä¼šå°†æ‰€æœ‰æ–‡æ¡£å—è½¬æ¢ä¸ºå‘é‡å¹¶å­˜å‚¨åœ¨å†…å­˜ä¸­ï¼Œç”¨äºåç»­çš„ç›¸ä¼¼æ€§æœç´¢
vector_store.add_documents(all_splits)

# 5. æ„å»ºç”¨æˆ·æŸ¥è¯¢
question = "DeepSeekæ ¸å¿ƒæŠ€æœ¯æ˜¯ä»€ä¹ˆï¼Ÿ"

# 6. åœ¨å‘é‡å­˜å‚¨ä¸­æœç´¢ç›¸å…³æ–‡æ¡£ï¼Œå¹¶å‡†å¤‡ä¸Šä¸‹æ–‡å†…å®¹
retrieved_docs = vector_store.similarity_search(question, k=3)
docs_content = "\n\n".join(doc.page_content for doc in retrieved_docs)

# 7. æ„å»ºæç¤ºæ¨¡æ¿
from langchain_core.prompts import ChatPromptTemplate

prompt = ChatPromptTemplate.from_template(
    """
                åŸºäºä»¥ä¸‹ä¸Šä¸‹æ–‡ï¼Œå›ç­”é—®é¢˜ã€‚å¦‚æœä¸Šä¸‹æ–‡ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œ
                è¯·è¯´"æˆ‘æ— æ³•ä»æä¾›çš„ä¸Šä¸‹æ–‡ä¸­æ‰¾åˆ°ç›¸å…³ä¿¡æ¯"ã€‚
                ä¸Šä¸‹æ–‡: {context}
                é—®é¢˜: {question}
                å›ç­”:"""
)

# 8. ä½¿ç”¨ollamaæœ¬åœ°å¤§è¯­è¨€æ¨¡å‹ç”Ÿæˆç­”æ¡ˆ
from langchain_ollama import ChatOllama # pip install langchain-ollama
llm = ChatOllama(
    model="qwen3:8b",  # å¯ä»¥æ ¹æ®éœ€è¦æ›´æ¢å…¶ä»–æ¨¡å‹ï¼Œå¦‚ llama2, mistral ç­‰
    request_timeout=300.0  # å¢åŠ è¶…æ—¶æ—¶é—´
)
answer = llm.invoke(prompt.format(question=question, context=docs_content))

# æ ¼å¼åŒ–è¾“å‡ºç­”æ¡ˆ
print("=" * 80)
print(f"ğŸ“ é—®é¢˜: {question}")
print("-" * 80)
print("ğŸ’¡ ç­”æ¡ˆ:")
print(answer.content if hasattr(answer, "content") else str(answer))
print("-" * 80)
print("ğŸ“š å‚è€ƒæ–‡æ¡£æ•°é‡:", len(retrieved_docs))
print("ğŸ” æ£€ç´¢åˆ°çš„ç›¸å…³ç‰‡æ®µ:")
for i, doc in enumerate(retrieved_docs, 1):
    print(f"\nç‰‡æ®µ {i}:")
    print(
        f"  å†…å®¹: {doc.page_content[:200]}{'...' if len(doc.page_content) > 200 else ''}"
    )
    if hasattr(doc, "metadata") and doc.metadata:
        print(f"  æ¥æº: {doc.metadata}")
print("=" * 80)

"""
LangChain RAGæ™ºèƒ½é—®ç­”ç³»ç»Ÿ - Ollamaæœ¬åœ°ç‰ˆæœ¬
ä½¿ç”¨ä¼ ç»Ÿçš„LangChainé“¾å¼è°ƒç”¨å®ç°æ£€ç´¢å¢å¼ºç”Ÿæˆ(RAG)
é€šè¿‡Ollamaè°ƒç”¨æœ¬åœ°éƒ¨ç½²çš„å¤§è¯­è¨€æ¨¡å‹ï¼Œæ— éœ€APIå¯†é’¥ï¼Œä¿æŠ¤æ•°æ®éšç§
é€‚åˆç¦»çº¿ç¯å¢ƒæˆ–å¯¹æ•°æ®å®‰å…¨æœ‰ä¸¥æ ¼è¦æ±‚çš„åœºæ™¯
"""
# ç¬¬ä¸€æ­¥ï¼šç´¢å¼•é˜¶æ®µ

# 1. åŠ è½½æ–‡æ¡£
# å¯¼å…¥å¿…è¦çš„æ¨¡å—å’Œç¯å¢ƒå˜é‡é…ç½®
import os
from dotenv import load_dotenv

# åŠ è½½.envæ–‡ä»¶ä¸­çš„ç¯å¢ƒå˜é‡ï¼ˆOllamaç‰ˆæœ¬é€šå¸¸ä¸éœ€è¦APIå¯†é’¥ï¼‰
load_dotenv()

# å¯¼å…¥ç½‘é¡µæ–‡æ¡£åŠ è½½å™¨ï¼Œç”¨äºä»ç½‘é¡µçˆ¬å–å†…å®¹
from langchain_community.document_loaders import WebBaseLoader

# åˆ›å»ºç½‘é¡µåŠ è½½å™¨å®ä¾‹ï¼ŒæŒ‡å®šè¦çˆ¬å–çš„URL
loader = WebBaseLoader(web_paths=("https://zh.wikipedia.org/wiki/æ·±åº¦æ±‚ç´¢",))  # æ·±åº¦æ±‚ç´¢çš„ç»´åŸºç™¾ç§‘é¡µé¢
docs = loader.load()  # æ‰§è¡ŒåŠ è½½æ“ä½œï¼Œè¿”å›Documentå¯¹è±¡åˆ—è¡¨

# 2. æ–‡æœ¬åˆ†å—
# å¯¼å…¥é€’å½’å­—ç¬¦æ–‡æœ¬åˆ†å‰²å™¨ï¼Œç”¨äºå°†é•¿æ–‡æœ¬åˆ‡åˆ†æˆå°å—
from langchain_text_splitters import RecursiveCharacterTextSplitter

# åˆ›å»ºæ–‡æœ¬åˆ†å‰²å™¨å®ä¾‹
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=1000,  # æ¯ä¸ªæ–‡æœ¬å—çš„æœ€å¤§å­—ç¬¦æ•°ï¼ˆå¹³è¡¡å†…å®¹å®Œæ•´æ€§å’Œå¤„ç†æ•ˆç‡ï¼‰
    chunk_overlap=200,  # ç›¸é‚»æ–‡æœ¬å—ä¹‹é—´çš„é‡å å­—ç¬¦æ•°ï¼ˆç¡®ä¿ä¿¡æ¯ä¸ä¸¢å¤±ï¼‰
)
# å°†åŠ è½½çš„æ–‡æœ¬åˆ†å‰²æˆå¤šä¸ªå°å—ï¼Œä¾¿äºå‘é‡åŒ–å’Œæ£€ç´¢
all_splits = text_splitter.split_documents(docs)

# 3. ä¿¡æ¯åµŒå…¥
# å¯¼å…¥HuggingFaceåµŒå…¥æ¨¡å‹æ¥å£
from langchain_huggingface import HuggingFaceEmbeddings

# åˆå§‹åŒ–ä¸­æ–‡åµŒå…¥æ¨¡å‹
# ä½¿ç”¨BAAI/bge-small-zhæ¨¡å‹ï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“é—¨é’ˆå¯¹ä¸­æ–‡ä¼˜åŒ–çš„åµŒå…¥æ¨¡å‹
# model_kwargsæŒ‡å®šä½¿ç”¨CPUè¿›è¡Œè®¡ç®—ï¼Œé€‚åˆæ²¡æœ‰GPUçš„ç¯å¢ƒ
# encode_kwargsä¸­çš„normalize_embeddings=Trueç¡®ä¿åµŒå…¥å‘é‡è¢«å½’ä¸€åŒ–ï¼Œæé«˜æ£€ç´¢ç²¾åº¦
embeddings = HuggingFaceEmbeddings(
    model_name="BAAI/bge-small-zh",  # ä¸­æ–‡åµŒå…¥æ¨¡å‹ï¼Œä½“ç§¯å°ä½†æ•ˆæœè‰¯å¥½
    model_kwargs={"device": "cpu"},   # ä½¿ç”¨CPUè®¾å¤‡è¿›è¡Œè®¡ç®—ï¼ˆå¯æ”¹ä¸º"cuda"ä½¿ç”¨GPUï¼‰
    encode_kwargs={"normalize_embeddings": True},  # å¯ç”¨å‘é‡å½’ä¸€åŒ–ï¼Œæå‡ç›¸ä¼¼åº¦è®¡ç®—å‡†ç¡®æ€§
)

# 4. å‘é‡å­˜å‚¨
# å¯¼å…¥å†…å­˜å‘é‡å­˜å‚¨ï¼Œç”¨äºå­˜å‚¨å’Œæ£€ç´¢æ–‡æ¡£å‘é‡
from langchain_core.vectorstores import InMemoryVectorStore

# åˆ›å»ºå‘é‡å­˜å‚¨å®ä¾‹ï¼Œä¼ å…¥åµŒå…¥æ¨¡å‹
vector_store = InMemoryVectorStore(embeddings)
# å°†åˆ†å‰²åçš„æ–‡æ¡£æ·»åŠ åˆ°å‘é‡å­˜å‚¨ä¸­
# è¿™ä¸€æ­¥ä¼šå°†æ‰€æœ‰æ–‡æ¡£å—è½¬æ¢ä¸ºå‘é‡å¹¶å­˜å‚¨åœ¨å†…å­˜ä¸­ï¼Œç”¨äºåç»­çš„ç›¸ä¼¼æ€§æœç´¢
vector_store.add_documents(all_splits)

# ç¬¬äºŒæ­¥ï¼šæ£€ç´¢é˜¶æ®µ

# 5. æ„å»ºç”¨æˆ·æŸ¥è¯¢
# å®šä¹‰è¦è¯¢é—®çš„é—®é¢˜ï¼ˆè¿™é‡Œå¯ä»¥æ”¹ä¸ºåŠ¨æ€è¾“å…¥ï¼‰
question = "DeepSeekæœ‰å“ªäº›æ ¸å¿ƒæŠ€æœ¯ï¼Ÿ"

# 6. åœ¨å‘é‡å­˜å‚¨ä¸­æœç´¢ç›¸å…³æ–‡æ¡£ï¼Œå¹¶å‡†å¤‡ä¸Šä¸‹æ–‡å†…å®¹
# ä½¿ç”¨ä½™å¼¦ç›¸ä¼¼åº¦æœç´¢æœ€ç›¸å…³çš„æ–‡æ¡£å—
retrieved_docs = vector_store.similarity_search(question, k=3)  # k=3è¡¨ç¤ºæ£€ç´¢å‰3ä¸ªæœ€ç›¸å…³çš„æ–‡æ¡£å—
# å°†æ£€ç´¢åˆ°çš„æ–‡æ¡£å†…å®¹æ‹¼æ¥æˆä¸€ä¸ªå­—ç¬¦ä¸²ï¼Œä½œä¸ºå¤§æ¨¡å‹çš„ä¸Šä¸‹æ–‡
docs_content = "\n\n".join(doc.page_content for doc in retrieved_docs)

# 7. æ„å»ºæç¤ºæ¨¡æ¿
# å¯¼å…¥èŠå¤©æç¤ºæ¨¡æ¿ï¼Œç”¨äºæ ¼å¼åŒ–ç”¨æˆ·é—®é¢˜å’Œæ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡
from langchain_core.prompts import ChatPromptTemplate

# åˆ›å»ºæç¤ºæ¨¡æ¿ï¼Œå®šä¹‰å¤§æ¨¡å‹çš„è§’è‰²å’Œä»»åŠ¡
prompt = ChatPromptTemplate.from_template(
    """åŸºäºä»¥ä¸‹ä¸Šä¸‹æ–‡ï¼Œè¯·è¯¦ç»†å›ç­”é—®é¢˜ã€‚å¦‚æœä¸Šä¸‹æ–‡ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œ
è¯·è¯´"æˆ‘æ— æ³•ä»æä¾›çš„ä¸Šä¸‹æ–‡ä¸­æ‰¾åˆ°ç›¸å…³ä¿¡æ¯"ã€‚

ä¸Šä¸‹æ–‡: {context}

é—®é¢˜: {question}

å›ç­”:"""
)

# ç¬¬ä¸‰æ­¥ï¼šç”Ÿæˆé˜¶æ®µ

# 8. ä½¿ç”¨ollamaæœ¬åœ°å¤§è¯­è¨€æ¨¡å‹ç”Ÿæˆç­”æ¡ˆ
# å¯¼å…¥OllamaèŠå¤©æ¨¡å‹ï¼Œç”¨äºè°ƒç”¨æœ¬åœ°éƒ¨ç½²çš„å¤§è¯­è¨€æ¨¡å‹
from langchain_ollama import ChatOllama  # éœ€è¦å®‰è£…: pip install langchain-ollama

# åˆå§‹åŒ–OllamaèŠå¤©æ¨¡å‹
# æ³¨æ„ï¼šä½¿ç”¨å‰éœ€è¦å…ˆé€šè¿‡å‘½ä»¤è¡Œå®‰è£…å¯¹åº”æ¨¡å‹ï¼Œä¾‹å¦‚: ollama pull qwen2.5:7b
llm = ChatOllama(
    model="qwen2.5:7b",  # æœ¬åœ°æ¨¡å‹åç§°ï¼Œå¯ä»¥æ ¹æ®éœ€è¦æ›´æ¢å…¶ä»–æ¨¡å‹ï¼ˆå¦‚llama3.1, mistral, codellamaç­‰ï¼‰
    request_timeout=300.0  # å¢åŠ è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰ï¼Œæœ¬åœ°æ¨¡å‹æ¨ç†å¯èƒ½éœ€è¦æ›´é•¿æ—¶é—´
    # temperature=0.7,  # å¯é€‰ï¼šæ§åˆ¶è¾“å‡ºéšæœºæ€§ï¼ŒæŸäº›Ollamaç‰ˆæœ¬æ”¯æŒ
    # num_ctx=4096,     # å¯é€‰ï¼šä¸Šä¸‹æ–‡çª—å£å¤§å°
)

# ä½¿ç”¨æç¤ºæ¨¡æ¿æ ¼å¼åŒ–é—®é¢˜å’Œä¸Šä¸‹æ–‡ï¼Œç„¶åè°ƒç”¨æœ¬åœ°å¤§æ¨¡å‹ç”Ÿæˆç­”æ¡ˆ
formatted_prompt = prompt.format(question=question, context=docs_content)
answer = llm.invoke(formatted_prompt)

# æ ¼å¼åŒ–è¾“å‡ºç­”æ¡ˆ
print("=" * 80)
print("ğŸ¤– LangChain RAG æ™ºèƒ½é—®ç­”ç³»ç»Ÿ (Ollamaæœ¬åœ°ç‰ˆæœ¬)")
print("=" * 80)
print(f"ğŸ“ é—®é¢˜: {question}")
print("-" * 80)
print("ğŸ’¡ ç­”æ¡ˆ:")
# å®‰å…¨åœ°æå–ç­”æ¡ˆå†…å®¹ï¼Œå…¼å®¹ä¸åŒçš„è¿”å›æ ¼å¼
print(answer.content if hasattr(answer, "content") else str(answer))
print("-" * 80)
print("ğŸ“š å‚è€ƒæ–‡æ¡£æ•°é‡:", len(retrieved_docs))
print("ğŸ” æ£€ç´¢åˆ°çš„ç›¸å…³ç‰‡æ®µ:")

# é€ä¸ªæ˜¾ç¤ºæ£€ç´¢åˆ°çš„æ–‡æ¡£ç‰‡æ®µï¼Œä¾¿äºç”¨æˆ·äº†è§£ç­”æ¡ˆæ¥æº
for i, doc in enumerate(retrieved_docs, 1):
    print(f"\nç‰‡æ®µ {i}:")
    # æˆªå–å‰200ä¸ªå­—ç¬¦æ˜¾ç¤ºï¼Œé¿å…è¾“å‡ºè¿‡é•¿
    content_preview = doc.page_content[:200]
    if len(doc.page_content) > 200:
        content_preview += "..."
    print(f"  å†…å®¹: {content_preview}")
    
    # å¦‚æœæ–‡æ¡£æœ‰å…ƒæ•°æ®ï¼ˆå¦‚æ¥æºURLï¼‰ï¼Œåˆ™æ˜¾ç¤º
    if hasattr(doc, "metadata") and doc.metadata:
        print(f"  æ¥æº: {doc.metadata}")

print("=" * 80)

# ä½¿ç”¨è¯´æ˜ï¼š
# 1. å®‰è£…Ollama: https://ollama.ai/
# 2. ä¸‹è½½æ¨¡å‹: ollama pull qwen2.5:7b
# 3. å¯åŠ¨OllamaæœåŠ¡: ollama serve
# 4. è¿è¡Œæ­¤è„šæœ¬
# 
# å¸¸ç”¨æ¨¡å‹æ¨èï¼š
# - qwen2.5:7b (é€šç”¨ä¸­æ–‡æ¨¡å‹ï¼Œå¹³è¡¡æ•ˆæœå’Œé€Ÿåº¦)
# - llama3.1:8b (è‹±æ–‡ä¸ºä¸»ï¼Œæ”¯æŒä¸­æ–‡)
# - mistral:7b (è½»é‡çº§æ¨¡å‹ï¼Œé€Ÿåº¦å¿«)
# - codellama:7b (ä»£ç ä¸“ç”¨æ¨¡å‹)
